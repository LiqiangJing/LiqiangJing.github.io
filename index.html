
<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta content="IE=7.0000" http-equiv="X-UA-Compatible">
<title>Liqiang Jing's Homepage</title>
<meta name="description" content="Liqian Jing.">
<meta name="keywords" content="Liqiang Jing, HFUT, SDU, UT Dallas, homepage, Ph.D.">

<style>@-moz-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-webkit-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-o-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}embed,object{animation-duration:.001s;-ms-animation-duration:.001s;-moz-animation-duration:.001s;-webkit-animation-duration:.001s;-o-animation-duration:.001s;animation-name:nodeInserted;-ms-animation-name:nodeInserted;-moz-animation-name:nodeInserted;-webkit-animation-name:nodeInserted;-o-animation-name:nodeInserted;}</style>	
	
<link rel="stylesheet" type="text/css" href="./files/weiyinwei.css">
<meta name="google-site-verification" content="OU-xCmiAYHXy1Aj5mcXXaFv9VjXH0fn1X__CmSR6dUg" />

</head>


<body>
<div id="content">
<!-- ============================NEWS===========================================!-->
<div id="news">
    <h2>News</h2><br>
    <font size="3px">

	<b>Nov 2025</b><br>
    <span class="easylink">
	Invited Keynote Speaker by CSAE 2026
	</span><br><br>
		
    <b>Oct 2025</b><br>
    <span class="easylink">
Our Paper Selected as a Best Paper Candidate at CIKM 2025
	</span><br><br>
		
		
    <b>Sep 2025</b><br>
    <span class="easylink">
    Received Lambda's Research Grant Program.
    </span><br><br>
		
		

		
    <b>July 2025</b><br>
    <span class="easylink">
    Our DSBench was used by <a href="https://openai.com/index/introducing-chatgpt-agent/" target="_blank">OpenAI for new ChatGPT agent evaluation</a>. Thanks for all coauthors!
    </span><br><br>
	    
    <b>July 2025</b><br>
    <span class="easylink">
    Invited to serve as a PC for AAAI 2026.
    </span><br><br>
	    
    <b>July 2025</b><br>
    <span class="easylink">
    Our new evaluation tool for Text2Video and Video2Text is out. Try our evaluation tool  <a href="https://github.com/du-nlp-lab/FIFA" target="_blank">FIFA</a>. 
    </span><br><br>
	    
    <b>June 2025</b><br>
    <span class="easylink">
    Invited to give a talk at NICE学术.
    </span><br><br>
	    
    <b>May 2025</b><br>
    <span class="easylink">
    Invited to give a talk by Rajhans Samdani and Anupam Datta at Snowflake.
    </span><br><br>
	    
    <b>May 2025</b><br>
    <span class="easylink">
    Invited to serve as the Area Chair for BMVC 2025.
    </span><br><br>

	    
    <b>May 2025</b><br>
    <span class="easylink">
    Our tutorial, Hallucinations in Large Language Models and Large Vision-Language Models, is accepted by ICMR 2025.
    </span><br><br>
	    
    <b>May 2025</b><br>
    <span class="easylink">
    I passed the Ph.D. qualification exam and became a Ph.D. candidate.
    </span><br><br>
	    

    <b>Apr 2025</b><br>
    <span class="easylink">
    I have been selected to receive the Louis Beecherl, Jr. Graduate Fellowship, which is a prestigious merit-based fellowship.
    </span><br><br>

	    
    <b>Apr 2025</b><br>
    <span class="easylink">
    Call for papers of our <a href="https://sites.google.com/view/ijcai-mklm" target="_blank">``The First Workshop on Multimodal Knowledge and Language Modeling''</a> is running. We welcome submissions from any relevant field.
    </span><br><br>

    <b>Apr 2025</b><br>
    <span class="easylink">
    Invited Editorial Board member for  <a href="https://ijai4s.org/index.php/journal/index" target="_blank">International Journal of Artificial Intelligence for Science (IJAI4S)</a>. Call for Papers is running.
    </span><br><br>
	    
    <b>Mar 2025</b><br>
    <span class="easylink">
    I won the ICLR 2025 Financial Assistance.
    </span><br><br>
	    
    <b>Mar 2025</b><br>
    <span class="easylink">
    Our workshop <a href="https://sites.google.com/view/ijcai-mklm" target="_blank">``The First Workshop on Multimodal Knowledge and Language Modeling''</a> was accepted to be held at IJCAI 2025.
    </span><br><br>
	    
    <b>Feb 2025</b><br>
    <span class="easylink">
    Our paper <b>  <a href="https://arxiv.org/abs/2412.14626" target="_blank">Learning to generate research idea</a> </b>  won the best paper award at AAAI AI4Research 2025.

    </span><br><br>


    <b>Feb 2025</b><br>
    <span class="easylink">
    We launched a startup <b>  <a href="https://tensorstax.com/" target="_blank">TensorStax</a> </b>  to pioneer Autonomous AI Data Engineer.
    </span><br><br>

<!--     <b>Feb 2025</b><br>
    <span class="easylink">
    I will join AWS AI as a 2025 summer intern, working on multimodal evaluation.
    </span><br><br> -->
	    
	    
    <b>Feb 2025</b><br>
    <span class="easylink">
    Invited Reviewer for COLM 2025.
    </span><br><br>

	    
    <b>Jan 2025</b><br>
    <span class="easylink">
    Invited Reviewer for CVPR 2025.
    </span><br><br>
	    
	<b>26 Dec 2024: We’re Engaged! </b><br>
    <span class="easylink">
    I’m thrilled to share that I got engaged to <a href="https://www.linkedin.com/in/yue-zhang-b44635287/" target="_blank">Yue Zhang</a>. This marks the beginning of an exciting new chapter in our lives, and we couldn’t be happier. 
	    Thank you to everyone who has supported us on this journey—we look forward to the adventures ahead!
    </span><br><br>
	    

    <b>Sep 2024</b><br>
    <span class="easylink">
    Our team (I am the Team Leader) has been selected as one of the 10 teams for the Amazon Trusted AI Challenge.
    </span><br><br>	    

	    
   <b>July 2024</b><br>
    <span class="easylink">
    Received a 4,000 USD research award from OpenAI.
    </span><br><br>	    

    <b>Feb 2024</b><br>
    <span class="easylink">
    I will be a Research Intern at Tencent AI Lab (Seattle) in May 2024
    </span><br><br>	    
	
   <b>Feb 2024</b><br>
    <span class="easylink">
    Invited Reviewer for ACM MM 2024. 
    </span><br><br>
	    
    <b>Jan 2024</b><br>
    <span class="easylink">
    Invited to serve on the review committee for the track LC10 - Information Extraction, Knowledge Extraction, and Text Mining at LREC-COLING 2024. 
    </span><br><br>
	    
    <b>Jan 2024</b><br>
    <span class="easylink">
    Invited Reviewer for ICML 2024.
    </span><br><br>
	    
<!--     <b>12 Dec 2023</b><br>
    <span class="easylink">
    One paper is accepted by ICASSP 2024.
    </span><br><br> -->
	    
<!--     <b>11 Dec 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Knowledge-enhanced Memory Model for Emotional Support Conversation"</b> is accepted as an oral paper by AAAI 2024 ML4CMH.
    </span><br><br> -->
	    
	    
<!--     <b>10 Dec 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Debiasing Multimodal Sarcasm Detection with Contrastive Learning"</b> is accepted by AAAI 2024.
    </span><br><br> -->
	    
    <b>Nov 2023</b><br>
    <span class="easylink">
    <b>  <a href="https://arxiv.org/pdf/2311.01477.pdf" target="_blank">FaithScore for LVLM hallucinations evaluation</a> </b> is out. Use our  <a href="https://pypi.org/project/faithscore/" target="_blank">tool</a> for any multimodal open-ended questions!
    </span><br><br>
	    
    <b>Sep 2023</b><br>
    <span class="easylink">
    Invited Reviewer for IEEE Transactions on Audio, Speech, and Language Processing and ICLR 2024.
    </span><br><br>
	    
<!--     <b>Sep 2023</b><br>
    <span class="easylink">
    Invited Reviewer for ICLR 2024.
    </span><br><br> -->
	    
    <b>Aug 2023</b><br>
    <span class="easylink">
    Invited Emergency Reviewer for EMNLP 2023.
    </span><br><br>
	    
<!--     <b>26 July 2023</b><br>
    <span class="easylink">
    Our full paper <b>"General Debiasing for Multimodal Sentiment Analysis"</b> is accepted by ACM MM 2023.
    </span><br><br> -->
	    
<!--     <b>14 Jun 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model"</b> is accepted by ACM TOIS.
    </span><br><br> -->
	    
<!--     <b>27 May 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain"</b> is accepted by ACM TOIS.
    </span><br><br> -->
	    
<!--     <b>2 May 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Multi-source Semantic Graph-based Multimodal Sarcasm Explanation Generation"</b> is accepted by ACL 2023.
    </span><br><br> -->
	    
<!--     <b>28 April 2023</b><br>
    <span class="easylink">
    I am very excited to pursue a PhD in Computer Science at the University of Texas at Dallas in the fall of 2023.
    </span><br><br> -->
	    
<!--     <b>20 April 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Dual Consistency-enhanced Semi-supervised Sentiment Analysis towards COVID-19 Tweets"</b> is accepted by IEEE TKDE.
    </span><br><br> -->
	    
<!--     <b>April 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Adapting Generative Pretrained Language Model for Open-domain Multimodal Sentence Summarization"</b> is accepted by <a href="https://sigir.org/sigir2023/" target="_blank">ACM SIGIR 2023</a>.
    </span><br><br> -->
	    
    <b>March 2023</b><br>
    <span class="easylink">
    Invited Reviewer for NeurIPS 2023.
    </span><br><br>
	    
    <b>February 2023</b><br>
    <span class="easylink">
    Invited Reviewer for IEEE TCSVT.
    </span><br><br>
	    
    <b>January 2023</b><br>
    <span class="easylink">
    Invited Reviewer for ICML 2023.
    </span><br><br>
	    
    <b>January 2023</b><br>
    <span class="easylink">
    Invited Reviewer for IEEE TMM.
    </span><br><br>
	    
<!--     <b>19 November 2022</b><br>
    <span class="easylink">
    Our paper <b>"Mutual-enhanced Incongruity Learning Network for Multi-modal Sarcasm Detection"</b> is accepted by <a href="https://aaai.org/Conferences/AAAI-23/" target="_blank">AAAI 2023 </a>
    </span><br><br> -->
	
    <b>October 2022</b><br>
    <span class="easylink">
    Invited Reviewer for Information Sciences.
    </span><br><br>
	    
<!--     <b>30 August 2022</b><br>
    <span class="easylink">
    Our paper <b>"Vision Enhanced Generative Pre-trained Language Model for Multimodal Sentence Summarization"</b> is accepted by <a href="https://www.mi-research.net/" target="_blank">Machine Intelligence Research</a>.
    </span><br><br> -->
	    
<!--     <b>30 July 2022</b><br>
    <span class="easylink">
    Our full paper <b>"CI-OCM: Counterfactural Inference towards Unbiased Outfit Compatibility Modeling"</b> is accepted by <a href="https://mcfr-mm22.github.io/" target="_blank">the Workshop MCFR of ACM MM 2022</a>.
    </span><br><br> -->
	    
    <b>July 2022</b><br>
    <span class="easylink">
    Invited Reviewer for NeurIPS 2022 and ACM MM 2022.
    </span><br><br>
	
<!--     <b>July 2022</b><br>
    <span class="easylink">
    Invited Reviewer for ACM MM 2022.
    </span><br><br> -->
	    
<!--     <b>June 2022</b><br>
    <span class="easylink">
    Our full paper <b>"Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis"</b> is accepted by <a href="https://2022.acmmm.org/" target="_blank">ACM MM 2022</a>.
    </span><br><br> -->
	    
    <b>April 2022</b><br>
    <span class="easylink">
    I win the <a href="https://sigir.org/sigir2022/" target="_blank">ACM SIGIR 2022</a> Student Travel Grant.
    </span><br><br>
	    
<!--     <b>31 March 2022</b><br>
    <span class="easylink">
    Our full paper <b>"V2P: Vision-to-Prompt based Multi-Modal Product Summary Generation"</b> is accepted by <a href="https://sigir.org/sigir2022/" target="_blank">ACM SIGIR 2022</a>.
    </span><br><br> -->
	    
   <b>August 2021</b><br>
    <span class="easylink">
    I will be a Research Intern at Alibaba DAMO Academy in Aug. 2021, supervised by Zhongzhou Zhao 
    </span><br><br>	    
    </font>
</div>
<!-- ============================Profiles===========================================!-->
<div id="left">
<table style="background-color:white;">
<tbody><tr nosave="">

	
<td valign="CENTER">
<img src="./images/jingliqiang.jpg" height="220" align="left">
<!-- <img src="./images/jingliqiang.jpg" height="220" align="left"> -->
</td>

<td valign="CENTER" align="left">
<font size="+0">
<b><font size="+2">Liqiang Jing （CN: 井立强）&nbsp;</font></b>
<p style="margin-left:0px;">
</p><p style="margin-left:0px;">
<b>Ph.D. Candidate</b>
</p><p style="margin-left:0px;">
<a href="https://x.com/jingliqiang6", target="_blank">Twitter</a><br/>
<a href="https://www.linkedin.com/in/liqiang-jing-a0547523b/", target="_blank">Linkedin</a><br/>
	
<!-- <a href="https://www.en.sdu.edu.cn/", target="_blank">Shandong University</a><br/> -->
<!-- </p><p style="margin-left:0px;">
The Hong Kong Polytechnic University, Hong Kong<br> -->
</p><p style="margin-left:0px;">
Email:&nbsp; jingliqiang6 AT gmail.com</a><br>
&bull; <a href="files/CV.pdf">CV</a> &bull; <a href="https://scholar.google.com/citations?hl=en&user=aNXRSOsAAAAJ">Google Scholar</a> &bull; <a href="https://github.com/LiqiangJing">GitHub</a> <br>
</p></font><p><font size="+0">
</font>
</p></td>
</tr>
</tbody></table>
<!-- supervised by  <a href="https://xinyadu.github.io/" target="\_blank">Prof. Xinya Du</a> -->
<div style="margin-top:20px;">
Liqiang Jing is a Ph.D. candidate at the University of Texas at Dallas supervised by  <a href="https://xinyadu.github.io/" target="\_blank">Prof. Xinya Du</a>. 
<!-- Liqiang Jing is a Ph.D. student at the University of Texas at Dallas.  -->
He received the Master degree in the School of Computer Science and Technology from Shandong University in 2023 (advised by <a href="https://xuemengsong.github.io/" target="\_blank">Prof. Xuemeng Song</a>  and <a href="https://liqiangnie.github.io/" target="\_blank"> Prof. Liqiang Nie</a>).
<!-- 	and the Bachelor degree in the School of Computer Science and Information from Hefei University of Technology in 2020.  -->
<!-- He also interned as a researcher at Alibaba Damo Academy, Tencent AI Lab, and AWS Bedrock.   -->
	His research interests include multimodal learning and natural language processing. Now, he is very interested in <b>Evaluation and Alignment for Large Vision-Language Models (LVLMs)/Large Language Models (LLMs), and LLMs/LVLMs as Agents for Real-world Tasks</b>.  
<!-- </div> -->
<!-- <div> -->
<!-- Liqiang Jing has published several papers in the top venues (e.g., ICLR, ACL, ACM SIGIR, ACM MM, AAAI, and IEEE TKDE).  -->
<!-- He is a reviewer for several top conferences and journals, such as NeurIPS, ICML, ICLR, TMM, TCSVT, and Information Sciences.  -->
<!-- 	He is the main organizer of The First Workshop on Multimodal Knowledge and Language Modeling (MKLM) held at IJCAI 2025. -->
<!-- 	and an editorial board member of International Journal of Artificial Intelligence for Science (IJAI4S). -->
<!-- 	He received the Best Paper Award from the AI4Research workshop held at AAAI 2025. He received an Award recommendation from ACM MM 2022. He received the Researcher Access Program Award from OpenAI. -->
<!-- Some of his research outputs have been integrated into the products of Alibaba, serving more than 200 companies (e.g., Givenchy, L 'Oreal and Nestle). 
	His <a href="https://github.com/bcdnlp/FAITHSCORE" target="_blank">multimodal evaluation tool</a> has been downloaded more than 
       <a href="https://pepy.tech/projects/faithscore"><img src="https://static.pepy.tech/badge/faithscore" alt="PyPI Downloads"></a>
	times and is widely used by academic institutions (e.g., UC Berkeley and UW) and industry (e.g., Intel Labs and Huggingface).  -->
</div>

	

	

<div>
	<p><font color='red'> 	<b>I am looking for full-time research scientist positions starting around the end of 2026 or early 2027. Please explore my profile and contact me if you have opportunities that match my goals.</b>
		<!-- I am happy to chat and discuss potential collaborations. Please feel free to reach out to me via Email (jingliqiang6 AT gmail.com). <br>  -->

		 <!-- - If you are looking for research opportunities in Professor Du's Lab, please contact Professor Xinya Du. <br>  -->

<!-- 		 - If you are a Beginner Researcher and want to work with me, please contact me with your CV, research interests, and research proposal. I won't take on an intern unless they have a project that interests me or I need an intern for my project. <br>  -->

		 <!-- - If you are a researcher and want to collaborate/talk with me, please contact me directly. -->
		</font></p>
</div>




	

<!-- ============================Education===========================================!-->
<h2 style="CLEAR: both;">Education</h2>
	<table>
  <tbody>
  <tr>
    <td><span class="title">The University of Texas at Dallas</span> <br>
	Ph.D. in Computer Science, Aug. 2023 - 2026 (Expected)  <br>
	Advisor: <a href="https://xinyadu.github.io/" target="_blank">Xinya Du</a> <br>
     </td>
   </tr>
   </tbody>
</table>     
	
<table>
  <tbody>
  <tr>
    <td><span class="title">Shandong University </span> <br>
	Master in Computer Technology, Sep. 2020 - Jun. 2023  <br>
	 <a href="papers/trans1.pdf" target="_blank">Transcript</a>  <br>
	Advisor: <a href="https://xuemengsong.github.io/" target="_blank">Xuemeng Song</a> <br>
	Co-Advisor: <a href="https://liqiangnie.github.io/index.html" target="_blank">Liqiang Nie</a> 
     </td>
   </tr>
   </tbody>
</table>     

<table>
  <tbody>
  <tr>
    <td><span class="title">Hefei University of Technology </span> <br>
	Bachelor in Computer Science and Technology, Sep. 2016 - Jul. 2020 <br>
	    <a href="papers/trans2.pdf" target="_blank">Transcript</a> <br>
     </td>
   </tr>
   </tbody>
</table>


<!-- ============================Experiences===========================================!-->
<h2 style="CLEAR: both">Experiences</h2>


	<table>
  <tbody><tr>
    <td> <span class="title">Applied Scientist Intern</span>, Amazon Web Services (AWS), Bedrock, Seattle, America, May. 2025 -- Aug 2025<br>
			Advisor: <a href="" target="_blank">Xiong Zhou</a>, 
	    <a href="" target="_blank">Evangelia Spiliopoulou</a>, <a href="" target="_blank">Siddharth Varia</a>, Neha Anna John, <a href="" target="_blank">Ioannidis Vassilis</a>
		</td></tr></tbody>
</table>

	

<table>
  <tbody><tr>
    <td> <span class="title">Research Advisor</span>, Tensorstax, San Francisco, America, Sep. 2024 -- Present <br>
		</td></tr></tbody>
</table>
	




	
	
<table>
  <tbody><tr>
    <td> <span class="title">Research Intern</span>, Tencent America AI Lab, Bellevue, America, May. 2024 -- Aug 2024<br>
			Advisor: <a href="https://scholar.google.com/citations?hl=en&user=EeppWmkAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Xiaoyang Wang</a>, 
	    <a href="https://wenlinyao.github.io/" target="_blank">Wenlin Yao</a>, <a href="https://wyu97.github.io/" target="_blank"> Wenhao Yu</a>, 
	    <a href="https://mayer123.github.io/" target="_blank">Kaixin Ma</a>, <a href="https://panda0881.github.io/Hongming_Homepage/" target="_blank">Hongming Zhang</a>, 
	    <a href="https://sites.google.com/view/dongyu888/" target="_blank">Dong Yu</a>
		</td></tr></tbody>
</table>

	
<table>
  <tbody><tr>
    <td> <span class="title">Research Intern</span>, Alibaba DAMO Academy, Hangzhou, China, May. 2022 -- Jan. 2023<br>
			Advisor:  <a href="https://dblp.uni-trier.de/pid/207/9948.html" target="_blank">Zhongzhou Zhao</a>
		</td></tr></tbody>
</table>

<table>
  <tbody><tr>
    <td> <span class="title">AIR <a href="https://damo.alibaba.com/collaborations/?lang=en" target="_blank">(Alibaba Innovative Research)</a> Project Intern</span>, Alibaba DAMO Academy, Hangzhou, China, Aug. 2021 -- Jan. 2022 <br>
			Advisor:  <a href="https://dblp.uni-trier.de/pid/207/9948.html" target="_blank">Zhongzhou Zhao</a>
		</td></tr></tbody>
</table>



<!-- ============================Experiences===========================================!-->


<!-- ============================Papers===========================================!-->

<!-- 	<div id="papers">
<h2 style="CLEAR: both">Preprints <a href="" target="_blank"></a></h2> 

	 -->

<div id="papers">
<h2 style="CLEAR: both">Organizer for Workshops/Tutorials<a href="" target="_blank"></a></h2> 
</br>

	<div>

	<table>
  <tbody>
	  
	  
<tr><td class="left"><a href="" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">The First Workshop on Multimodal Knowledge and Language Modeling.</span> 
      <br><b>Liqiang Jing</b>, Xinya Du, Fei Hao, Manling Li, Aixin Sun & William Yang Wang
    <br> IJCAI 2025
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Slides</a> &nbsp;&nbsp;
&nbsp;&nbsp;&bull; <a href="https://sites.google.com/view/ijcai-mklm" target="_blank">Website</a> &nbsp;&nbsp;
  </td>
</tr>
	  

	  <tr><td class="left"><a href="https://dl.acm.org/doi/10.1145/3731715.3734581" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">Tutorial: Hallucinations in Large Language Models and Large Vision-Language Models.</span> 
      <br><b>Liqiang Jing</b>, Yue Zhang & Xinya Du
    <br> ICMR 2025
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Slides</a> &nbsp;&nbsp;
&nbsp;&nbsp;&bull; <a href="https://www.icmr-2025.org/programs/tutorials/" target="_blank">Website</a> &nbsp;&nbsp;
  </td>
</tr>

	  
 </tbody>
</table>

	
		
<div id="papers">
<h2 style="CLEAR: both">Papers<a href="" target="_blank"></a></h2> 
</br>

	<div>
	<p>* indicates equal contribution. † indicates that I am a project leader or co-leader. </p>
</div>
	<b> Preprints: </b> </br></br>


	<table>
  <tbody>

	  	  <tr><td class="left"><a href="https://arxiv.org/abs/2412.14626" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[A1] Learning to Generate Research Idea with Dynamic Control.</span> 
      <br>Ruochen Li, <b>Liqiang Jing</b>, Chi Han, Jiawei Zhou & Xinya Du 
    <br> Arxiv <a href="https://openreview.net/forum?id=zCb0dPvGYN" target="_blank"><font color='red'>(Best Paper Award at AAAI AI4Research)</font> </a> 
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>


<tr><td class="left"><a href="https://arxiv.org/abs/2504.02876" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[A2] Multimodal Reference Visual Grounding.</span> 
      <br>Yangxiao Lu, Ruosen Li, <b>Liqiang Jing</b>, Jikai Wang, Xinya Du, Yunhui Guo, Nicholas Ruozzi & Yu Xiang
    <br> Arxiv 
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>



	  	  <tr><td class="left"><a href="https://arxiv.org/abs/2507.06523" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[A3] FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation.</span> 
      <br> <b>Liqiang Jing</b>, Viet Lai, Seunghyun Yoon, Trung Bui & Xinya Du 
    <br> Arxiv <a href="" target="_blank"> </a> 
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>	  
	  


	  	  	  <tr><td class="left"><a href="https://arxiv.org/abs/2509.11866" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[A4] Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding.</span> 
      <br> Meng Luo, Shengqiong Wu, <b>Liqiang Jing</b>, Tianjie Ju, Li Zheng, Jinxiang Lai, Tianlong Wu, Xinya Du, Jian Li, Siyuan Yan, Jiebo Luo, William Yang Wang, Hao Fei, Mong-Li Lee & Wynne Hsu    <br> Arxiv <a href="" target="_blank"> </a> 
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>	

	  
 </tbody>
</table>


<b> In the Year of 2025: </b> </br></br>
<table>
  <tbody> 


	  	  	  <tr><td class="left"><a href="https://arxiv.org/abs/2506.17335" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[25] LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research.</span> 
      <br>Shuo Yan, Ruochen Li, Ziming Luo, Zimu Wang, Daoyang Li, <b>Liqiang Jing</b>†, Kaiyu He, Peilin Wu, George Michalopoulos, Yue Zhang, Ziyang Zhang, Mian Zhang, Zhiyu Chen & Xinya Du 
    <br> EMNLP 2025 <a href="" target="_blank"></a> 
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>
	  

	  	  	  	  	  <!-- <tr><td class="left"><a href="https://arxiv.org/abs/2505.01958" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[24] A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models.</span> 
      <br><b>Liqiang Jing</b>*, Guiming Hardy Chen*, Ehsan Aghazadeh, Xin Eric Wang & Xinya Du
    <br> ACL 2025 
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr> -->
	  



	  
	  	  	  <tr><td class="left"><a href="https://arxiv.org/abs/2409.13612" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[23] FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs.</span> 
      <br><b>Liqiang Jing</b>*, Bowen Yan*, Zhengsong Zhang*, Eftekhar Hossain & Xinya Du
    <br> ACL 2025
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>

	  <tr><td class="left"><a href="https://www.arxiv.org/abs/2508.03654" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[22] Can Large Vision-Language Models Understand Multimodal Sarcasm?</span> 
      <br>Xinyu Wang*, Yue Zhang* & <b>Liqiang Jing</b>†
    <br> CIKM 2025  <font color='red'>(Best Paper Candidates)</font>
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>
	  
	  
	  <tr><td class="left"><a href="https://arxiv.org/abs/2404.05046" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[21] FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback.</span> 
      <br><b>Liqiang Jing</b> & Xinya Du
    <br> TMLR 2025
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>

	  
  <tr>    
    <td class="left"><a href="https://arxiv.org/abs/2412.16232" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">[20] Defeasible Visual Entailment: Benchmark, Evaluator, and Reward-Driven Optimization.</span> 
      <br> Yue Zhang, <b>Liqiang Jing</b> & Vibhav Gogate
    <br>AAAI 2025
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
  </tr>

	  <tr><td class="left"><a href="" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[19] Fine-grained and Explanable Factuality Evaluation for Multimodal Summarization.</span> 
      <br>Yue Zhang, Jingxuan Zuo & <b>Liqiang Jing</b>†
    <br> AAAI 2025
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>
	 

	  <tr><td class="left"><a href="https://arxiv.org/abs/2402.03658" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[18] Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue.</span> 
      <br>Kun Ouyang, <b>Liqiang Jing</b>†, Xuemeng Song, Meng Liu, Yupeng Hu & Liqiang Nie
    <br> IEEE TMM
   &nbsp;&nbsp;&bull; <a href="https://arxiv.org/abs/2402.11414" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>

	  	  <tr><td class="left"><a href="https://arxiv.org/abs/2409.07703" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[17] DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?</span> 
      <br><b>Liqiang Jing</b>, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du & Dong Yu
    <br> ICLR 2025  <a href="https://huggingface.co/papers/2409.07703" target="_blank"><font color='red'>(HuggingFace #1 Paper of the day,</font></a> <a href="https://openai.com/index/introducing-chatgpt-agent/" target="_blank"><font color='red'> used by OpenAI for ChatGPT agent evaluation)</font></a>
   &nbsp;&nbsp;&bull; <a href="https://liqiangjing.github.io/dsbench.github.io/" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>

	  
 </tbody>
</table>
	

<b> In the Year of 2024: </b> </br></br>
<table>
  <tbody>  

	  
  <tr>    
    <td class="left"><a href="https://arxiv.org/abs/2312.10493" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">[16] Debiasing Multimodal Sarcasm Detection with Contrastive Learning.</span> 
      <br> Mengzhao Jia, Can Xie & <b>Liqiang Jing</b>†
    <br>AAAI 2024
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
  </tr>

	  <tr><td class="left"><a href="https://arxiv.org/abs/2310.07700" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[15] Knowledge-enhanced Memory Model for Emotional Support Conversation.</span> 
      <br>Mengzhao Jia, Qianglong Chen, <b>Liqiang Jing</b>, Dawei Fu & Renyu Li
    <br> AAAI 2024
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>


	  	  <tr><td class="left"><a href="https://arxiv.org/abs/2312.10210" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[14] VK-G2T: Vision and Context Knowledge Enhanced Gloss2Text.</span> 
      <br> <b>Liqiang Jing</b>, Xuemeng Song, Xinxing Zu, Na Zheng, Zhongzhou Zhao & Liqiang Nie
    <br> ICASSP 2024
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>

	   <tr><td class="left"><a href="https://arxiv.org/abs/2409.16494" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[13] A Unified Hallucination Mitigation Framework for Large Vision-Language Models.</span> 
      <br><b>Liqiang Jing</b>*, Yue Chang*, Xiaopeng Zhang* & Yue Zhang
    <br> TMLR
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>

	  <tr><td class="left"><a href="files/ARR_June_faithscore.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[12] FaithScore: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models.</span> 
      <br><b>Liqiang Jing</b>, Ruosen Li, Yunmo Chen & Xinya Du
    <br> EMNLP 2024
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>  
	  
  
 </tbody>
</table>
	

	
	<b> In the Year of 2023: </b> </br></br>
<table>
  <tbody>
	<tr>
	  <td class="left"><a href="https://link.springer.com/article/10.1007/s11633-022-1372-x" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">[11] Vision Enhanced Generative Pre-trained Language Model for Multimodal Sentence Summarization.</span> 
      <br> <b>Liqiang Jing</b>, Yiren Li, Junhao Xu, Yongcan Yu, Pei Shen & Xuemeng Song
    <br>Machine Intelligence Research
   &nbsp;&nbsp;&bull; <a href="https://github.com/LiqiangJing/Vision-GPLM" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
  </tr>  
	  
	  <tr>
	<td class="left"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/26138" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">[10] Mutual-enhanced Incongruity Learning Network for Multi-modal Sarcasm Detection.</span> 
      <br>Yang Qiao,  <b>Liqiang Jing</b>†, Xuemeng Song, Xiaolin Chen, Lei Zhu & Liqiang Nie
    <br>AAAI 2023 <font color='red'>(Oral)</font>
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
	  </tr>
    
	  <tr>
	<td class="left"><a href="https://dl.acm.org/doi/abs/10.1145/3539618.3591633" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">[9] Adapting Generative Pretrained Language Model for Open-domain Multimodal Sentence Summarization.</span> 
      <br>Dengtian Lin,  <b>Liqiang Jing</b>†, Xuemeng Song, Meng Liu, Teng Sun & Liqiang Nie
    <br>ACM SIGIR 2023 <font color='red'>(Oral)</font>
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
	  </tr>
	  
	  	  <tr>
	<td class="left"><a href="https://ieeexplore.ieee.org/document/10109879" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">[8] Dual Consistency-enhanced Semi-supervised Sentiment Analysis towards COVID-19 Tweets.</span> 
      <br>Teng Sun,  <b>Liqiang Jing</b>, Yinwei Wei, Xuemeng Song, Zhiyong Cheng & Liqiang Nie
    <br>IEEE TKDE
   &nbsp;&nbsp;&bull; <a href="https://ieeexplore.ieee.org/document/10109879" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
	  </tr>
	  

	  	  	  <tr>
	<td class="left"><a href="https://aclanthology.org/2023.acl-long.635/" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">[7] Multi-source Semantic Graph-based Multimodal Sarcasm Explanation Generation.</span> 
      <br><b>Liqiang Jing</b>, Xuemeng Song, Kun Ouyang, Mengzhao Jia & Liqiang Nie
    <br>ACL 2023
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
	  </tr>
	  
	<tr><td class="left"><a href="https://dl.acm.org/doi/10.1145/3603374" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[6] Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain.</span> 
      <br><b>Liqiang Jing</b>, Xuemeng Song, Xuming Lin, Zhongzhou Zhao, Wei Zhou & Liqiang Nie
    <br> ACM TOIS
   &nbsp;&nbsp;&bull; <a href="https://dl.acm.org/doi/10.1145/3603374" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>
	  
	<tr><td class="left"><a href="https://dl.acm.org/doi/pdf/10.1145/3606368" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[5] Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model.</span> 
      <br>Xiaolin Chen, Xuemeng Song, <b>Liqiang Jing</b>, Shuo Li, Linmei Hu & Liqiang Nie
    <br>ACM TOIS
   &nbsp;&nbsp;&bull; <a href="https://multimodaldialog.wixsite.com/website" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>

	<tr><td class="left"><a href="https://dl.acm.org/doi/10.1145/3581783.3612051" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">[4] General Debiasing for Multimodal Sentiment Analysis.</span> 
      <br>Teng Sun, Juntong Ni, Wenjie Wang, <b>Liqiang Jing</b>, Yinwei Wei & Liqiang Nie
    <br>ACM MM 2023
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>
	  
	  
	  
 </tbody>
</table>




<b> In the Year of 2022: </b> </br></br>
<table>
  <tbody>  
  <tr>    
    <td class="left"><a href="https://dl.acm.org/doi/abs/10.1145/3477495.3532076" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">[3] V2P: Vision-to-Prompt based Multi-Modal Product Summary Generation.</span> 
      <br> Xuemeng Song, <b>Liqiang Jing</b>, Dengtian Lin, Zhongzhou Zhao, Haiqing Chen & Liqiang Nie
    <br>ACM SIGIR 2022 <font color='red'>(Oral)</font>
   &nbsp;&nbsp;&bull; <a href="https://xuemengsong.github.io/V2P_Code.rar" target="_blank">Codes&Data</a> &nbsp;&nbsp;

  </td>
  </tr>
	 
	    <tr>
	      <td class="left"><a href="https://dl.acm.org/doi/10.1145/3503161.3548211" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">[2] Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis.</span> 
      <br> Teng Sun, Wenjie Wang, <b>Liqiang Jing</b>, Yiran Cui, Xuemeng Song & Liqiang Nie
    <br>ACM MM 2022 <font color='red'>(<a href="https://openreview.net/forum?id=qGLcjNnQ_QX&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Dacmmm.org%2FACMMM%2F2022%2FConference%2FAuthors%23your-papers)" target="_blank"><font color='red'>Area Chair Award Recommendation,</font></a> Oral)</font>
   &nbsp;&nbsp;&bull; <a href="https://github.com/Teng-Sun/CLUE_model" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
  </tr>
	     <tr>
	   <td class="left"><a href="https://dl.acm.org/doi/abs/10.1145/3552468.3555363" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">[1] CI-OCM: Counterfactural Inference towards Unbiased Outfit Compatibility Modeling.</span> 
      <br> <b>Liqiang Jing</b>, Minghui Tian, Xiaolin Chen, Teng Sun, Weili Guan & Xuemeng Song
    <br>ACM MM 2022
   &nbsp;&nbsp;&bull; <a href="https://github.com/LiqiangJing/CI-OCM" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
  </tr>
  
 </tbody>
</table>



<!-- ============================Patent===========================================!-->
<!-- <h2 style="CLEAR: both;">Patent</h2> -->




<!-- </td></tr></tbody></table> -->


	<h2 style="CLEAR: both;">Research Summary</h2>

	<table>
  <tbody>
  <tr>
	<br>	
In the early stages of my research, I focused on developing deep learning models—including BERT, Graph Neural Networks, and Convolutional Neural Networks—to solve specific NLP and multimodal tasks such as text sentiment analysis, controllable and stylized text generation, multimodal sentiment analysis, summarization, sarcasm detection, and sarcasm rationale generation. I also explored semi-supervised learning approaches to address low-resource challenges in NLP. <br><br>
More recently, I have shifted my focus to the evaluation, analysis, and optimization of Large Vision-Language Models (LVLMs). This includes designing new benchmarks and evaluation metrics to assess their capabilities, and employing post-training techniques (e.g., supervised fine-tuning and reinforcement learning) and synthetic data generation to enhance their ability to perform complex and reliable reasoning. In parallel, I am investigating ways to evaluate and improve agent systems, particularly in tasks such as code generation, data science workflows, and automated scientific discovery. <br><br>

		Some of my research outputs have been integrated into the products of Alibaba, serving more than 200 companies (e.g., Givenchy, L 'Oreal and Nestle). 
	My <a href="https://github.com/bcdnlp/FAITHSCORE" target="_blank">multimodal hallucination evaluation tool</a> has been downloaded more than 
       <a href="https://pepy.tech/projects/faithscore"><img src="https://static.pepy.tech/badge/faithscore" alt="PyPI Downloads"></a>
	times and is widely used by academic institutions (e.g., UC Berkeley and UW) and industry (e.g., Intel Labs and Huggingface). My work DSBench was <a href="https://openai.com/index/introducing-chatgpt-agent/" target="_blank">used by OpenAI for their new ChatGPT agent.</a> 
He has received numerous honors and grants, including selection as a Best Short Paper Candidate at ACM CIKM 2025, the AAAI AI4Research Best Paper Award (2025), OpenAI Researcher Access Program awards (2024, 2025), and the Lambda Research Grant Program (2025).
	
   </tr>
   </tbody>
</table>   

	

<h2 style="CLEAR: both;">Talks</h2>



<table><tbody><tr><td>
  <span class="title"> FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs. NICE学术 62期. Virtual. June, 2025. </span> &nbsp;&nbsp; 
  <br> <a href="https://docs.google.com/presentation/d/1weMMqJdhSa97vO8Kc2YOgQ9cjwYispRSLrtVnxCcCsY/edit?usp=sharing" target="_blank">Slides</a> 
</td></tr></tbody></table>

	
<table><tbody><tr><td>
  <span class="title">Factuality Evaluation for Multimodal Summarization on <a href="https://sites.google.com/view/docui-aaai25/" target="_blank">The AAAI-25 Workshop on Document Understanding and Intelligence</a> Philadelphia, Pennsylvania, USA. Mar, 2025. </span> &nbsp;&nbsp; 
  <br> <a href="" target="_blank">Slides</a> 
</td></tr></tbody></table>
	
	<table><tbody><tr><td>
  <span class="title">Evaluating Hallucination in LVLMs on <a href="https://superagi.com/agi-leap-summit/" target="_blank">AGI Super Summit</a> hosted by SuperAGI. Feb, 2024. </span> &nbsp;&nbsp; 
  <br> <a href="" target="_blank">Slides</a> 
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">ACM MM 2022 Oral on Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis. Oct, 2022.</span> &nbsp;&nbsp; 
  <br> <a href="papers/msa1.pdf" target="_blank">Slides</a>
</td></tr></tbody></table>
	
<table><tbody><tr><td>
  <span class="title">ACM MCFR 2022 on CI-OCM: Counterfactual Inference towards Unbiased Outfit Compatibility Modeling. Oct, 2022.</span> &nbsp;&nbsp; 
  <br> <a href="papers/ocm1.pdf" target="_blank">Slides</a>
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">ACM SIGIR 2022 Oral on V2P: Vision-to-Prompt based Multi-Modal Product Summary Generation. Jul, 2022. </span> &nbsp;&nbsp; 
  <br> <a href="papers/v2p1.pdf" target="_blank">Slides</a>
</td></tr></tbody></table>

<!-- ============================Patent===========================================!-->

<h2 style="CLEAR: both;">Patent</h2>

<table><tbody><tr><td>
  <span class="title">Text generation methods and devices (applying) CN202211048016.0 &nbsp;&nbsp;
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">Text generation methods and devices (applying) CN202211158537.1 &nbsp;&nbsp;
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title"> A knowledge-guided multi-source information fusion method for predicting blast furnace gas (applying) CN202210606561.0 &nbsp;&nbsp;
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">Adaptive deployment methods, systems, devices and storage media of transmission inspection terminals CN202211146873.4 &nbsp;&nbsp;
</td></tr></tbody></table>

<!-- ============================TA===========================================!-->
<h2 style="CLEAR: both;">Teaching Assistant</h2>
<table><tbody><tr><td>
	<span class="title">Spring 2024, CS 4375 <a href="https://xinyadu.github.io/cs4375/index.html" target="_blank">Introduction to Machine Learning</a>, University of Texas at Dallas</span><br><br>

	<span class="title">Fall 2023, CS 6320 <a href="https://xinyadu.github.io/cs6320/index.html" target="_blank">Natural Language Processing</a>, University of Texas at Dallas</span><br><br>

  <span class="title">Spring 2021, Machine Learning, Shandong University</span><br><br>

</tbody></table>


<!-- 	<h2 style="CLEAR: both;">Mentored Interns</h2>
<table><tbody><tr><td>
	<span class="title"> It is a great pleasure to work with such talented young people.
I am grateful for the trust that they have placed in me and for the support that I have received from my advisors. </span> <br> <be>
	
		 <a href="https://www.linkedin.com/in/zeyupan/" target="_blank">Zeyu Pan</a>, Undergraduate in Tsinghua University -> Master in UCLA<br><be>
		 <a href="https://eftekhar-hossain.github.io/" target="_blank">Eftekhar Hossain</a>, Assistant Professor in CUET<br><be>
		 <a href="https://scholar.google.com/citations?hl=en&user=ysSewR8AAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Ehsan Aghazadeh</a>, Ph.D. in UMass<br><be>
	 <a href="" target="_blank">Xiaopeng Zhang</a>, submitted 1 paper to ARR, Undergraduate in Harbin Institute of Technology<br><be>
	 <a href="" target="_blank">Yue Chang</a>, submitted 1 paper to ARR, Undergraduate in Harbin Institute of Technology<br><be>
	 <a href="https://for4ward.github.io/" target="_blank">Jingxuan Zuo</a>, submitted 1 paper to ARR, Undergraduate in Shandong University<br><be>
	<a href="https://ouyangkun10.github.io/" target="_blank">Kun Ouyang</a>, published 1 paper at ACL 2023 and submitted 1 paper to
	ACM TMM, Undergraduate in Shandong University -> Ph.D. in Peking University<br><be>
	Can Xie, published 1 paper at AAAI 2024, Undergraduate in Shandong University -> Ph.D. in Institute of Automation, Chinese Academy of Sciences<br><be>
	<a href="https://lingfenggold.github.io" target="_blank">Juntong Ni</a>, published 1 paper at ACM MM 2023, Undergraduate in Shandong University <br><be>
	Yiran Cui, published 1 paper at ACM MM 2022, Undergraduate in Shandong University -> Ruoyu Tec<br><be>
	<a href="https://scholar.google.com/citations?user=9UTMYbkAAAAJ&hl=en" target="_blank">Yongcan Yu</a>, published 1 paper at Machine Intelligence Research, Undergraduate in Shandong University -> Master in Institute of Automation, Chinese Academy of Sciences<br><be>
	Junhao Xu, published 1 paper at Machine Intelligence Research, Undergraduate in Shandong University -> Master in Fudan University<br><be>
	Dongwei Li, Undergraduate in Shandong University -> Master in Rice University<br><be>
	Dengtian Lin, published 1 paper at ACM SIGIR 2022 and 1 paper at ACM SIGIR 2023, Master in Shandong University<br><be>
	Yang Qiao, published 1 paper at AAAI 2023, Master in Shandong University<br><be>
	Minghui Tian, published 1 paper at ACM MM 2022, Undergraduate in Shandong University -> Master in Shandong University -> Ph.D. in Shandong University<br><be>

<!-- 			<span class="title"><a href="https://ouyangkun10.github.io/" target="_blank">Kun Ouyang</a>, published 1 paper at ACL 2023 and submitted 1 paper to
	ACM TMM, Shandong University -> Peking University</span><br><be>
	<span class="title">Can Xie, published 1 paper at AAAI 2024, Shandong University -> Institute of Automation, Chinese Academy of Sciences</span><br><be>
	<span class="title"><a href="lingfenggold.github.io" target="_blank">Juntong Ni</a>, published 1 paper at ACM MM 2023, Shandong University </span><br><be>
	<span class="title">Yiran Cui, published 1 paper at ACM MM 2022, Shandong University -> Ruoyu Tec</span><br><be>
	<span class="title"><a href="https://scholar.google.com/citations?user=9UTMYbkAAAAJ&hl=en" target="_blank">Yongcan Yu</a>, published 1 paper at Machine Intelligence Research, Shandong University -> Institute of Automation, Chinese Academy of Sciences</span><br><be>
	<span class="title">Junhao Xu, published 1 paper at Machine Intelligence Research, Shandong University -> Fudan University</span><br><be>
	<span class="title">Dongwei Li, Shandong University -> Rice University</span><br><be>
	<span class="title">Dongtian Lin, published 1 paper at ACM SIGIR 2022 and 1 paper at ACM SIGIR 2023, Shandong University</span><br><be>
	<span class="title">Yang Qiao, published 1 paper at AAAI 2023, Shandong University </span><br><be>
	<span class="title">Minghui Tian, published 1 paper at ACM MM 2022, Shandong University </span><br><be> -->

<!-- </tbody></table>  -->
	

<!-- ============================Professional Service===========================================!-->
<h2 style="CLEAR: both;">Professional Services</h2>

<table><tbody>

	<tr><td>
	<b>Area Chair for Conferences: <br>
		
		&nbsp;&nbsp;&nbsp;&nbsp;2025: BMVC. </b>

	</td></tr>

	
	<tr><td>
	<b>Organizer: 
		<br> &nbsp;&nbsp;&nbsp;&nbsp;The First Workshop on Multimodal Knowledge and Language Modeling at IJCAI 2025.<br> 
		&nbsp;&nbsp;&nbsp;&nbsp;Tutorial on Hallucinations in Large Language Models and Large Vision-Language Models at ICMR 2025.</b>

	</td></tr>
	
	<tr><td>
	<b>Invited Reviewer for Conferences: <br>
		
		&nbsp;&nbsp;&nbsp;&nbsp;2022: ACM MM Workshop, NeurIPS; <br>
		
		&nbsp;&nbsp;&nbsp;&nbsp;2023: ICML, NeurIPS, EMNLP; <br>
		
		&nbsp;&nbsp;&nbsp;&nbsp;2024: ICLR, ICML, LREC, COLING, ACM MM. <br> 
		&nbsp;&nbsp;&nbsp;&nbsp;2025: ACL ARR Feb, NeurIPS. </b>

	</td></tr>
	
	<tr><td>
	<b>Invited Reviewer for Journals: <br> 
		&nbsp;&nbsp;&nbsp;&nbsp; Information Sciences, IEEE TMM, IEEE TCSVT, IEEE TASLP, Neurocomputing.</b>
	</td></tr>
	
</tbody></table>


<!-- ============================Honors===========================================!-->
<h2 style="CLEAR: both;">Honors</h2>
<table><tbody><tr><td>
	  <span class="title"> Best Paper Candidates, CIKM, 2025</span><br><br>

	<!-- <span class="title">Anthropic AI Safety Fellowship - Final Round (~100/2000+ globally), 2025</span><br><br> -->

	<span class="title">Researcher Access Program Award, OpenAI, 2025</span><br><br>

	  <span class="title">Lambda's Research Grant Program, 2025</span><br><br>

  <span class="title">Louis Beecherl, Jr. Graduate Fellowship, 2025</span><br><br>

  <span class="title">Financial Assistance, ICLR, 2025</span><br><br>

  <span class="title">Best Paper Award, AAAI AI4Research, 2025</span><br><br>
  <span class="title">Amazon Trusted AI Challenge – Global Top 10 Selection (Team Leader), 2024</span><br><br>

  <span class="title">Researcher Access Program Award, OpenAI, 2024</span><br><br>
  <span class="title">Outstanding Graduate Student, Shandong University, 2022</span><br><br>

  <span class="title">ACM SIGIR Student Travel Grant, 2022</span><br><br>

  <span class="title">Excellent Graduate, Hefei University of Technology, 2020</span><br><br>

  <span class="title">National Encouragement Scholarship, 2017, 2018, 2019</span><br><br>

  <span class="title">First Class Scholarship, Hefei University of Technology, 2018, 2019</span><br><br>

</tbody></table>
<!-- ============================Invited Talks===========================================!-->
</br>		

<!-- ============================Map===========================================!-->
<a href="https://clustrmaps.com/site/1bpzi"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=vuHwEKQroohZZfj1UB99qZ70x6e6FgHjRjzZ1ukd3I0&cl=ffffff" /></a>
<!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=vuHwEKQroohZZfj1UB99qZ70x6e6FgHjRjzZ1ukd3I0&cl=ffffff&w=a"></script> -->
<!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=300&t=tt&d=P5t3EabrzZY8aFh3ZhuRPAXXUh7jCpV3TVHKUlqbMjA&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script> -->
<!-- ============================Map===========================================! -->

</br>		
<p>Last Update, Nov 2025.</p> 
<!-- 	<p>Webpage template borrows from <a href="https://weiyinwei.github.io/">Yinwei Wei</a>.</p>  -->

</div>
</div>

</body>
</html>

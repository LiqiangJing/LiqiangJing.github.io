
<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta content="IE=7.0000" http-equiv="X-UA-Compatible">
<title>Liqiang Jing's Homepage</title>
<meta name="description" content="Liqian Jing.">
<meta name="keywords" content="Liqiang Jing, HFUT, SDU, UT Dallas, homepage, Ph.D.">

<style>@-moz-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-webkit-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-o-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}embed,object{animation-duration:.001s;-ms-animation-duration:.001s;-moz-animation-duration:.001s;-webkit-animation-duration:.001s;-o-animation-duration:.001s;animation-name:nodeInserted;-ms-animation-name:nodeInserted;-moz-animation-name:nodeInserted;-webkit-animation-name:nodeInserted;-o-animation-name:nodeInserted;}</style>	
	
<link rel="stylesheet" type="text/css" href="./files/weiyinwei.css">
<meta name="google-site-verification" content="OU-xCmiAYHXy1Aj5mcXXaFv9VjXH0fn1X__CmSR6dUg" />

</head>


<body>
<div id="content">
<!-- ============================NEWS===========================================!-->
<div id="news">
    <h2>News</h2><br>
    <font size="3px">


    <b>Sep 2024</b><br>
    <span class="easylink">
    Our team (I am the Team Leader) has been selected as one of the 10 teams for the Amazon Trusted AI Challenge.
    </span><br><br>	    

	    
   <b>July 2024</b><br>
    <span class="easylink">
    Received a 4,000 USD research award from OpenAI.
    </span><br><br>	    

    <b>Feb 2024</b><br>
    <span class="easylink">
    I will be a Research Intern at Tencent AI Lab (Seattle) in May 2024
    </span><br><br>	    
	
   <b>Feb 2024</b><br>
    <span class="easylink">
    Invited Reviewer for ACM MM 2024. 
    </span><br><br>
	    
    <b>Jan 2024</b><br>
    <span class="easylink">
    Invited to serve on the review committee for the track LC10 - Information Extraction, Knowledge Extraction, and Text Mining at LREC-COLING 2024. 
    </span><br><br>
	    
    <b>Jan 2024</b><br>
    <span class="easylink">
    Invited Reviewer for ICML 2024.
    </span><br><br>
	    
<!--     <b>12 Dec 2023</b><br>
    <span class="easylink">
    One paper is accepted by ICASSP 2024.
    </span><br><br> -->
	    
<!--     <b>11 Dec 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Knowledge-enhanced Memory Model for Emotional Support Conversation"</b> is accepted as an oral paper by AAAI 2024 ML4CMH.
    </span><br><br> -->
	    
	    
<!--     <b>10 Dec 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Debiasing Multimodal Sarcasm Detection with Contrastive Learning"</b> is accepted by AAAI 2024.
    </span><br><br> -->
	    
    <b>Nov 2023</b><br>
    <span class="easylink">
    <b>  <a href="https://arxiv.org/pdf/2311.01477.pdf" target="_blank">FaithScore for LVLM hallucinations evaluation</a> </b> is out. Use our  <a href="https://pypi.org/project/faithscore/" target="_blank">tool</a> for any multimodal open-ended questions!
    </span><br><br>
	    
    <b>Sep 2023</b><br>
    <span class="easylink">
    Invited Reviewer for IEEE Transactions on Audio, Speech, and Language Processing and ICLR 2024.
    </span><br><br>
	    
<!--     <b>Sep 2023</b><br>
    <span class="easylink">
    Invited Reviewer for ICLR 2024.
    </span><br><br> -->
	    
    <b>Aug 2023</b><br>
    <span class="easylink">
    Invited Emergency Reviewer for EMNLP 2023.
    </span><br><br>
	    
<!--     <b>26 July 2023</b><br>
    <span class="easylink">
    Our full paper <b>"General Debiasing for Multimodal Sentiment Analysis"</b> is accepted by ACM MM 2023.
    </span><br><br> -->
	    
<!--     <b>14 Jun 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model"</b> is accepted by ACM TOIS.
    </span><br><br> -->
	    
<!--     <b>27 May 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain"</b> is accepted by ACM TOIS.
    </span><br><br> -->
	    
<!--     <b>2 May 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Multi-source Semantic Graph-based Multimodal Sarcasm Explanation Generation"</b> is accepted by ACL 2023.
    </span><br><br> -->
	    
<!--     <b>28 April 2023</b><br>
    <span class="easylink">
    I am very excited to pursue a PhD in Computer Science at the University of Texas at Dallas in the fall of 2023.
    </span><br><br> -->
	    
<!--     <b>20 April 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Dual Consistency-enhanced Semi-supervised Sentiment Analysis towards COVID-19 Tweets"</b> is accepted by IEEE TKDE.
    </span><br><br> -->
	    
<!--     <b>April 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Adapting Generative Pretrained Language Model for Open-domain Multimodal Sentence Summarization"</b> is accepted by <a href="https://sigir.org/sigir2023/" target="_blank">ACM SIGIR 2023</a>.
    </span><br><br> -->
	    
    <b>March 2023</b><br>
    <span class="easylink">
    Invited Reviewer for NeurIPS 2023.
    </span><br><br>
	    
    <b>February 2023</b><br>
    <span class="easylink">
    Invited Reviewer for IEEE TCSVT.
    </span><br><br>
	    
    <b>January 2023</b><br>
    <span class="easylink">
    Invited Reviewer for ICML 2023.
    </span><br><br>
	    
    <b>January 2023</b><br>
    <span class="easylink">
    Invited Reviewer for IEEE TMM.
    </span><br><br>
	    
<!--     <b>19 November 2022</b><br>
    <span class="easylink">
    Our paper <b>"Mutual-enhanced Incongruity Learning Network for Multi-modal Sarcasm Detection"</b> is accepted by <a href="https://aaai.org/Conferences/AAAI-23/" target="_blank">AAAI 2023 </a>
    </span><br><br> -->
	
    <b>October 2022</b><br>
    <span class="easylink">
    Invited Reviewer for Information Sciences.
    </span><br><br>
	    
<!--     <b>30 August 2022</b><br>
    <span class="easylink">
    Our paper <b>"Vision Enhanced Generative Pre-trained Language Model for Multimodal Sentence Summarization"</b> is accepted by <a href="https://www.mi-research.net/" target="_blank">Machine Intelligence Research</a>.
    </span><br><br> -->
	    
<!--     <b>30 July 2022</b><br>
    <span class="easylink">
    Our full paper <b>"CI-OCM: Counterfactural Inference towards Unbiased Outfit Compatibility Modeling"</b> is accepted by <a href="https://mcfr-mm22.github.io/" target="_blank">the Workshop MCFR of ACM MM 2022</a>.
    </span><br><br> -->
	    
    <b>July 2022</b><br>
    <span class="easylink">
    Invited Reviewer for NeurIPS 2022 and ACM MM 2022.
    </span><br><br>
	
<!--     <b>July 2022</b><br>
    <span class="easylink">
    Invited Reviewer for ACM MM 2022.
    </span><br><br> -->
	    
<!--     <b>June 2022</b><br>
    <span class="easylink">
    Our full paper <b>"Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis"</b> is accepted by <a href="https://2022.acmmm.org/" target="_blank">ACM MM 2022</a>.
    </span><br><br> -->
	    
    <b>April 2022</b><br>
    <span class="easylink">
    I win the <a href="https://sigir.org/sigir2022/" target="_blank">ACM SIGIR 2022</a> Student Travel Grant.
    </span><br><br>
	    
<!--     <b>31 March 2022</b><br>
    <span class="easylink">
    Our full paper <b>"V2P: Vision-to-Prompt based Multi-Modal Product Summary Generation"</b> is accepted by <a href="https://sigir.org/sigir2022/" target="_blank">ACM SIGIR 2022</a>.
    </span><br><br> -->
	    
   <b>August 2021</b><br>
    <span class="easylink">
    I will be as a Research Intern at Alibaba DAMO Academy in Aug. 2021, supervised by Zhongzhou Zhao 
    </span><br><br>	    
    </font>
</div>
<!-- ============================Profiles===========================================!-->
<div id="left">
<table style="background-color:white;">
<tbody><tr nosave="">

	
<td valign="CENTER">
<img src="./images/profile.png" height="220" align="left">
<!-- <img src="./images/jingliqiang.jpg" height="220" align="left"> -->
</td>

<td valign="CENTER" align="left">
<font size="+0">
<b><font size="+2">Liqiang Jing （CN: 井立强）&nbsp;</font></b>
<p style="margin-left:0px;">
</p><p style="margin-left:0px;">
<!--<b>PHD Student</b>
</p><p style="margin-left:0px;">-->
<!-- <a href="http://ilearn.qd.sdu.edu.cn/", target="_blank">iLEARN</a><br/> -->
<!-- <a href="https://www.en.sdu.edu.cn/", target="_blank">Shandong University</a><br/> -->
<!-- </p><p style="margin-left:0px;">
The Hong Kong Polytechnic University, Hong Kong<br> -->
</p><p style="margin-left:0px;">
Email:&nbsp; jingliqiang6 AT gmail.com</a><br>
&bull; <a href="files/CV.pdf">CV</a> &bull; <a href="https://scholar.google.com/citations?hl=en&user=aNXRSOsAAAAJ">Google Scholar</a> &bull; <a href="https://github.com/LiqiangJing">GitHub</a> <br>
</p></font><p><font size="+0">
</font>
</p></td>
</tr>
</tbody></table>
<!-- supervised by  <a href="https://xinyadu.github.io/" target="\_blank">Prof. Xinya Du</a> -->
<div style="margin-top:20px;">
Liqiang Jing is a Ph.D. student at the University of Texas at Dallas supervised by  <a href="https://xinyadu.github.io/" target="\_blank">Prof. Xinya Du</a>. 
<!-- Liqiang Jing is a Ph.D. student at the University of Texas at Dallas.  -->
He received the Master degree in the School of Computer Science and Technology from Shandong University in 2023 (advised by <a href="https://xuemengsong.github.io/" target="\_blank">Prof. Xuemeng Song</a>  and <a href="https://liqiangnie.github.io/" target="\_blank"> Prof. Liqiang Nie</a>) and the Bachelor degree in the School of Computer Science and Information from Hefei University of Technology 
in 2020. His research interests include multimodal learning and natural language processing. 

Now, he is very interested in <b>Evaluation and Alignment for Large Vision-Language Models (LVLMs)/Large Language Models (LLMs), and LLMs/LVLMs as Agents for Real-world Tasks</b>.
<!-- He has published several papers in the top venues (e.g., ACM SIGIR, ACM MM, AAAI, ACL, and IEEE TKDE). 
Moreover, he has served as a reviewer for several top conferences and journals, such as NeurIPS, TMM, ICML, TCSVT, ICLR, and Information Sciences. -->
	
</div>
<!-- ============================Profiles===========================================!-->

<div>
	<p><font color='red'> I am happy to chat and discuss potential collaborations. Please feel free to reach out to me via Email (jingliqiang6 AT gmail.com). <br> 

		 - If you are looking for research opportunities in Professor Du's Lab, please contact Professor Xinya Du. <br> 

		 - If you are a Beginner Researcher and want to work with me, please contact me with your CV, research interests, and research proposal. I won't take on an intern unless they have a project that interests me or I need an intern for my project. <br> 

		 - If you are an experienced researcher and want to collaborate/talk with me, please contact me directly.
		</font></p>
</div>


<!-- ============================Education===========================================!-->
<h2 style="CLEAR: both;">Education</h2>
	<table>
  <tbody>
  <tr>
    <td><span class="title">The University of Texas at Dallas</span> <br>
	Ph.D. in Computer Science, Aug. 2023 - present  <br>
	Advisor: <a href="https://xinyadu.github.io/" target="_blank">Xinya Du</a> <br>
     </td>
   </tr>
   </tbody>
</table>     
	
<table>
  <tbody>
  <tr>
    <td><span class="title">Shandong University </span> <br>
	Master in Computer Technology, Sep. 2020 - Jun. 2023  <br>
	 <a href="papers/trans1.pdf" target="_blank">Transcript</a>  <br>
	Advisor: <a href="https://xuemengsong.github.io/" target="_blank">Xuemeng Song</a> <br>
	Co-Advisor: <a href="https://liqiangnie.github.io/index.html" target="_blank">Liqiang Nie</a> 
     </td>
   </tr>
   </tbody>
</table>     

<table>
  <tbody>
  <tr>
    <td><span class="title">Hefei University of Technology </span> <br>
	Bachelor in Computer Science and Technology, Sep. 2016 - Jul. 2020 <br>
	    <a href="papers/trans2.pdf" target="_blank">Transcript</a> <br>
     </td>
   </tr>
   </tbody>
</table>


<!-- ============================Experiences===========================================!-->
<h2 style="CLEAR: both">Experiences</h2>
	
<table>
  <tbody><tr>
    <td> <span class="title">Research Intern</span>, Tencent America AI Lab, Bellevue, America, May. 2024 -- Aug 2024<br>
			Advisor: <a href="https://scholar.google.com/citations?hl=en&user=EeppWmkAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Xiaoyang Wang</a>, 
	    <a href="https://wenlinyao.github.io/" target="_blank">Wenlin Yao</a>, <a href="https://wyu97.github.io/" target="_blank"> Wenhao Yu</a>, 
	    <a href="https://mayer123.github.io/" target="_blank">Kaixin Ma</a>, <a href="https://panda0881.github.io/Hongming_Homepage/" target="_blank">Hongming Zhang</a>, 
	    <a href="https://sites.google.com/view/dongyu888/" target="_blank">Dong Yu</a>
		</td></tr></tbody>
</table>
	
<table>
  <tbody><tr>
    <td> <span class="title">Research Intern</span>, Alibaba DAMO Academy, Hangzhou, China, May. 2022 -- Jan. 2023<br>
			Advisor:  <a href="https://dblp.uni-trier.de/pid/207/9948.html" target="_blank">Zhongzhou Zhao</a>
		</td></tr></tbody>
</table>

<table>
  <tbody><tr>
    <td> <span class="title">AIR <a href="https://damo.alibaba.com/collaborations/?lang=en" target="_blank">(Alibaba Innovative Research)</a> Project Intern</span>, Alibaba DAMO Academy, Hangzhou, China, Aug. 2021 -- Jan. 2022 <br>
			Advisor:  <a href="https://dblp.uni-trier.de/pid/207/9948.html" target="_blank">Zhongzhou Zhao</a>
		</td></tr></tbody>
</table>



<!-- ============================Experiences===========================================!-->


<!-- ============================Papers===========================================!-->

<!-- 	<div id="papers">
<h2 style="CLEAR: both">Preprints <a href="" target="_blank"></a></h2> 

	 -->

		
		
<div id="papers">
<h2 style="CLEAR: both">Papers<a href="" target="_blank"></a></h2> 
</br>

	<div>
	<p>* indicates equal contribution. † indicates that I am a project leader or co-leader. </p>
</div>
	<b> Preprints: </b> </br></br>


	<table>
  <tbody>

<tr><td class="left"><a href="https://arxiv.org/abs/2402.03658" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue.</span> 
      <br>Kun Ouyang, <b>Liqiang Jing</b>†, Xuemeng Song, Meng Liu, Yupeng Hu & Liqiang Nie
    <br> Arxiv
   &nbsp;&nbsp;&bull; <a href="https://arxiv.org/abs/2402.11414" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>
	  
<tr><td class="left"><a href="" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">Fine-grained and Explanable Factuality Evaluation for Multimodal Summarization.</span> 
      <br>Yue Zhang, Jingxuan Zuo & <b>Liqiang Jing</b>†
    <br> Arxiv
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>
	  
<tr><td class="left"><a href="https://arxiv.org/abs/2404.05046" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback.</span> 
      <br><b>Liqiang Jing</b> & Xinya Du
    <br> Arxiv
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>

	  <tr><td class="left"><a href="https://arxiv.org/abs/2409.07703" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?</span> 
      <br><b>Liqiang Jing</b>, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du & Dong Yu
    <br> Arxiv
   &nbsp;&nbsp;&bull; <a href="https://liqiangjing.github.io/dsbench.github.io/" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>

	  	  <tr><td class="left"><a href="https://arxiv.org/abs/2409.13612" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs.</span> 
      <br>Bowen Yan*, Zhengsong Zhang*, <b>Liqiang Jing</b>*, Eftekhar Hossain & Xinya Du
    <br> Arxiv
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>
	  
	  
 </tbody>
</table>


<b> In the Year of 2025: </b> </br></br>
<table>
  <tbody>  
  <tr>    
    <td class="left"><a href="" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Defeasible Visual Entailment: Benchmark, Evaluator, and Reward-Driven Optimization.</span> 
      <br> Yue Zhang, <b>Liqiang Jing</b> & Vibhav Gogate
    <br>AAAI 2025
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;

  </td>
  </tr>
	 
  
 </tbody>
</table>
	

<b> In the Year of 2024: </b> </br></br>
<table>
  <tbody>  

	  
  <tr>    
    <td class="left"><a href="https://arxiv.org/abs/2312.10493" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Debiasing Multimodal Sarcasm Detection with Contrastive Learning.</span> 
      <br> Mengzhao Jia, Can Xie & <b>Liqiang Jing</b>†
    <br>AAAI 2024
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
  </tr>

	  <tr><td class="left"><a href="https://arxiv.org/abs/2310.07700" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">Knowledge-enhanced Memory Model for Emotional Support Conversation.</span> 
      <br>Mengzhao Jia, Qianglong Chen, <b>Liqiang Jing</b>, Dawei Fu & Renyu Li
    <br> AAAI 2024
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>


	  	  <tr><td class="left"><a href="https://arxiv.org/abs/2312.10210" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">VK-G2T: Vision and Context Knowledge Enhanced Gloss2Text.</span> 
      <br> <b>Liqiang Jing</b>, Xuemeng Song, Xinxing Zu, Na Zheng, Zhongzhou Zhao & Liqiang Nie
    <br> ICASSP 2024
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>

	   <tr><td class="left"><a href="https://arxiv.org/abs/2409.16494" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">A Unified Hallucination Mitigation Framework for Large Vision-Language Models.</span> 
      <br>Yue Chang*, <b>Liqiang Jing</b>*, Xiaopeng Zhang* & Yue Zhang
    <br> TMLR
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>

	  <tr><td class="left"><a href="files/ARR_June_faithscore.pdf" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">FaithScore: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models.</span> 
      <br><b>Liqiang Jing</b>, Ruosen Li, Yunmo Chen & Xinya Du
    <br> EMNLP 2024
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>  
	  
  
 </tbody>
</table>
	

	
	<b> In the Year of 2023: </b> </br></br>
<table>
  <tbody>
	<tr>
	  <td class="left"><a href="https://link.springer.com/article/10.1007/s11633-022-1372-x" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Vision Enhanced Generative Pre-trained Language Model for Multimodal Sentence Summarization.</span> 
      <br> <b>Liqiang Jing</b>, Yiren Li, Junhao Xu, Yongcan Yu, Pei Shen & Xuemeng Song
    <br>Machine Intelligence Research
   &nbsp;&nbsp;&bull; <a href="https://github.com/LiqiangJing/Vision-GPLM" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
  </tr>  
	  
	  <tr>
	<td class="left"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/26138" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Mutual-enhanced Incongruity Learning Network for Multi-modal Sarcasm Detection.</span> 
      <br>Yang Qiao,  <b>Liqiang Jing</b>†, Xuemeng Song, Xiaolin Chen, Lei Zhu & Liqiang Nie
    <br>AAAI 2023 <font color='red'>(Oral)</font>
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
	  </tr>
    
	  <tr>
	<td class="left"><a href="https://dl.acm.org/doi/abs/10.1145/3539618.3591633" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Adapting Generative Pretrained Language Model for Open-domain Multimodal Sentence Summarization.</span> 
      <br>Dengtian Lin,  <b>Liqiang Jing</b>†, Xuemeng Song, Meng Liu, Teng Sun & Liqiang Nie
    <br>ACM SIGIR 2023 <font color='red'>(Oral)</font>
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
	  </tr>
	  
	  	  <tr>
	<td class="left"><a href="https://ieeexplore.ieee.org/document/10109879" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Dual Consistency-enhanced Semi-supervised Sentiment Analysis towards COVID-19 Tweets.</span> 
      <br>Teng Sun,  <b>Liqiang Jing</b>, Yinwei Wei, Xuemeng Song, Zhiyong Cheng & Liqiang Nie
    <br>IEEE TKDE
   &nbsp;&nbsp;&bull; <a href="https://ieeexplore.ieee.org/document/10109879" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
	  </tr>
	  

	  	  	  <tr>
	<td class="left"><a href="https://aclanthology.org/2023.acl-long.635/" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Multi-source Semantic Graph-based Multimodal Sarcasm Explanation Generation.</span> 
      <br><b>Liqiang Jing</b>, Xuemeng Song, Kun Ouyang, Mengzhao Jia & Liqiang Nie
    <br>ACL 2023
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
	  </tr>
	  
	<tr><td class="left"><a href="https://dl.acm.org/doi/10.1145/3603374" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain.</span> 
      <br><b>Liqiang Jing</b>, Xuemeng Song, Xuming Lin, Zhongzhou Zhao, Wei Zhou & Liqiang Nie
    <br> ACM TOIS
   &nbsp;&nbsp;&bull; <a href="https://dl.acm.org/doi/10.1145/3603374" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>
	  
	<tr><td class="left"><a href="https://dl.acm.org/doi/pdf/10.1145/3606368" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model.</span> 
      <br>Xiaolin Chen, Xuemeng Song, <b>Liqiang Jing</b>, Shuo Li, Linmei Hu & Liqiang Nie
    <br>ACM TOIS
   &nbsp;&nbsp;&bull; <a href="https://multimodaldialog.wixsite.com/website" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>

	<tr><td class="left"><a href="https://dl.acm.org/doi/10.1145/3581783.3612051" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">General Debiasing for Multimodal Sentiment Analysis.</span> 
      <br>Teng Sun, Juntong Ni, Wenjie Wang, <b>Liqiang Jing</b>, Yinwei Wei & Liqiang Nie
    <br>ACM MM 2023
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>
	  
	  
	  
 </tbody>
</table>




<b> In the Year of 2022: </b> </br></br>
<table>
  <tbody>  
  <tr>    
    <td class="left"><a href="https://dl.acm.org/doi/abs/10.1145/3477495.3532076" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">V2P: Vision-to-Prompt based Multi-Modal Product Summary Generation.</span> 
      <br> Xuemeng Song, <b>Liqiang Jing</b>, Dengtian Lin, Zhongzhou Zhao, Haiqing Chen & Liqiang Nie
    <br>ACM SIGIR 2022 <font color='red'>(Oral)</font>
   &nbsp;&nbsp;&bull; <a href="https://xuemengsong.github.io/V2P_Code.rar" target="_blank">Codes&Data</a> &nbsp;&nbsp;

  </td>
  </tr>
	 
	    <tr>
	      <td class="left"><a href="https://dl.acm.org/doi/10.1145/3503161.3548211" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis.</span> 
      <br> Teng Sun, Wenjie Wang, <b>Liqiang Jing</b>, Yiran Cui, Xuemeng Song & Liqiang Nie
    <br>ACM MM 2022 <font color='red'>(Award Recommendation, Oral)</font>
   &nbsp;&nbsp;&bull; <a href="https://github.com/Teng-Sun/CLUE_model" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
  </tr>
	     <tr>
	   <td class="left"><a href="https://dl.acm.org/doi/abs/10.1145/3552468.3555363" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">CI-OCM: Counterfactural Inference towards Unbiased Outfit Compatibility Modeling.</span> 
      <br> <b>Liqiang Jing</b>, Minghui Tian, Xiaolin Chen, Teng Sun, Weili Guan & Xuemeng Song
    <br>ACM MM 2022
   &nbsp;&nbsp;&bull; <a href="https://github.com/LiqiangJing/CI-OCM" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
  </tr>
  
 </tbody>
</table>



<!-- ============================Patent===========================================!-->
<!-- <h2 style="CLEAR: both;">Patent</h2> -->







</td></tr></tbody></table>

<h2 style="CLEAR: both;">Talks</h2>

	<table><tbody><tr><td>
  <span class="title">Evaluating Hallucination in LVLMs on <a href="https://superagi.com/agi-leap-summit/" target="_blank">AGI Super Summit</a> hosted by SuperAGI. Feb, 2024. </span> &nbsp;&nbsp; 
  <br> <a href="" target="_blank">Slides</a> 
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">ACM MM 2022 Oral on Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis. Oct, 2022.</span> &nbsp;&nbsp; 
  <br> <a href="papers/msa1.pdf" target="_blank">Slides</a>
</td></tr></tbody></table>
	
<table><tbody><tr><td>
  <span class="title">ACM MCFR 2022 on CI-OCM: Counterfactual Inference towards Unbiased Outfit Compatibility Modeling. Oct, 2022.</span> &nbsp;&nbsp; 
  <br> <a href="papers/ocm1.pdf" target="_blank">Slides</a>
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">ACM SIGIR 2022 Oral on V2P: Vision-to-Prompt based Multi-Modal Product Summary Generation. Jul, 2022. </span> &nbsp;&nbsp; 
  <br> <a href="papers/v2p1.pdf" target="_blank">Slides</a>
</td></tr></tbody></table>

<!-- ============================Patent===========================================!-->

<h2 style="CLEAR: both;">Patent</h2>

<table><tbody><tr><td>
  <span class="title">Text generation methods and devices (applying) CN202211048016.0 &nbsp;&nbsp;
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">Text generation methods and devices (applying) CN202211158537.1 &nbsp;&nbsp;
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title"> A knowledge-guided multi-source information fusion method for predicting blast furnace gas (applying) CN202210606561.0 &nbsp;&nbsp;
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">Adaptive deployment methods, systems, devices and storage media of transmission inspection terminals CN202211146873.4 &nbsp;&nbsp;
</td></tr></tbody></table>

<!-- ============================TA===========================================!-->
<h2 style="CLEAR: both;">Teaching Assistant</h2>
<table><tbody><tr><td>
	<span class="title">Spring 2024, CS 4375 <a href="https://xinyadu.github.io/cs4375/index.html" target="_blank">Introduction to Machine Learning</a>, University of Texas at Dallas</span><br><br>

	<span class="title">Fall 2023, CS 6320 <a href="https://xinyadu.github.io/cs6320/index.html" target="_blank">Natural Language Processing</a>, University of Texas at Dallas</span><br><br>

  <span class="title">Spring 2021, Machine Learning, Shandong University</span><br><br>

</tbody></table>


<!-- 	<h2 style="CLEAR: both;">Mentored Interns</h2>
<table><tbody><tr><td>
	<span class="title"> It is a great pleasure to work with such talented young people.
I am grateful for the trust that they have placed in me and for the support that I have received from my advisors. </span> <br> <be>
	
		 <a href="https://www.linkedin.com/in/zeyupan/" target="_blank">Zeyu Pan</a>, Undergraduate in Tsinghua University -> Master in UCLA<br><be>
		 <a href="https://eftekhar-hossain.github.io/" target="_blank">Eftekhar Hossain</a>, Assistant Professor in CUET<br><be>
		 <a href="https://scholar.google.com/citations?hl=en&user=ysSewR8AAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Ehsan Aghazadeh</a>, Ph.D. in UMass<br><be>
	 <a href="" target="_blank">Xiaopeng Zhang</a>, submitted 1 paper to ARR, Undergraduate in Harbin Institute of Technology<br><be>
	 <a href="" target="_blank">Yue Chang</a>, submitted 1 paper to ARR, Undergraduate in Harbin Institute of Technology<br><be>
	 <a href="https://for4ward.github.io/" target="_blank">Jingxuan Zuo</a>, submitted 1 paper to ARR, Undergraduate in Shandong University<br><be>
	<a href="https://ouyangkun10.github.io/" target="_blank">Kun Ouyang</a>, published 1 paper at ACL 2023 and submitted 1 paper to
	ACM TMM, Undergraduate in Shandong University -> Ph.D. in Peking University<br><be>
	Can Xie, published 1 paper at AAAI 2024, Undergraduate in Shandong University -> Ph.D. in Institute of Automation, Chinese Academy of Sciences<br><be>
	<a href="https://lingfenggold.github.io" target="_blank">Juntong Ni</a>, published 1 paper at ACM MM 2023, Undergraduate in Shandong University <br><be>
	Yiran Cui, published 1 paper at ACM MM 2022, Undergraduate in Shandong University -> Ruoyu Tec<br><be>
	<a href="https://scholar.google.com/citations?user=9UTMYbkAAAAJ&hl=en" target="_blank">Yongcan Yu</a>, published 1 paper at Machine Intelligence Research, Undergraduate in Shandong University -> Master in Institute of Automation, Chinese Academy of Sciences<br><be>
	Junhao Xu, published 1 paper at Machine Intelligence Research, Undergraduate in Shandong University -> Master in Fudan University<br><be>
	Dongwei Li, Undergraduate in Shandong University -> Master in Rice University<br><be>
	Dengtian Lin, published 1 paper at ACM SIGIR 2022 and 1 paper at ACM SIGIR 2023, Master in Shandong University<br><be>
	Yang Qiao, published 1 paper at AAAI 2023, Master in Shandong University<br><be>
	Minghui Tian, published 1 paper at ACM MM 2022, Undergraduate in Shandong University -> Master in Shandong University -> Ph.D. in Shandong University<br><be>

<!-- 			<span class="title"><a href="https://ouyangkun10.github.io/" target="_blank">Kun Ouyang</a>, published 1 paper at ACL 2023 and submitted 1 paper to
	ACM TMM, Shandong University -> Peking University</span><br><be>
	<span class="title">Can Xie, published 1 paper at AAAI 2024, Shandong University -> Institute of Automation, Chinese Academy of Sciences</span><br><be>
	<span class="title"><a href="lingfenggold.github.io" target="_blank">Juntong Ni</a>, published 1 paper at ACM MM 2023, Shandong University </span><br><be>
	<span class="title">Yiran Cui, published 1 paper at ACM MM 2022, Shandong University -> Ruoyu Tec</span><br><be>
	<span class="title"><a href="https://scholar.google.com/citations?user=9UTMYbkAAAAJ&hl=en" target="_blank">Yongcan Yu</a>, published 1 paper at Machine Intelligence Research, Shandong University -> Institute of Automation, Chinese Academy of Sciences</span><br><be>
	<span class="title">Junhao Xu, published 1 paper at Machine Intelligence Research, Shandong University -> Fudan University</span><br><be>
	<span class="title">Dongwei Li, Shandong University -> Rice University</span><br><be>
	<span class="title">Dongtian Lin, published 1 paper at ACM SIGIR 2022 and 1 paper at ACM SIGIR 2023, Shandong University</span><br><be>
	<span class="title">Yang Qiao, published 1 paper at AAAI 2023, Shandong University </span><br><be>
	<span class="title">Minghui Tian, published 1 paper at ACM MM 2022, Shandong University </span><br><be> -->

<!-- </tbody></table>  -->
	

<!-- ============================Professional Service===========================================!-->
<h2 style="CLEAR: both;">Professional Services</h2>

<table><tbody>
	<tr><td>
	<b>Invited Reviewer for Conferences: <br>
		
		&nbsp;&nbsp;&nbsp;&nbsp;2022: ACM MM Workshop, NeurIPS; <br>
		
		&nbsp;&nbsp;&nbsp;&nbsp;2023: ICML, NeurIPS, EMNLP; <br>
		
		&nbsp;&nbsp;&nbsp;&nbsp;2024: ICLR, ICML, LREC, COLING, ACM MM. </b>
	</td></tr>
	<tr><td>
	<b>Invited Reviewer for Journals: <br> 
		&nbsp;&nbsp;&nbsp;&nbsp; Information Sciences, IEEE TMM, IEEE TCSVT, IEEE TASLP, Neurocomputing.</b>
	</td></tr>
</tbody></table>


<!-- ============================Honors===========================================!-->
<h2 style="CLEAR: both;">Honors</h2>
<table><tbody><tr><td>
  <span class="title">Researcher Access Program Award, OpenAI, 2024</span><br><br>

  <span class="title">Outstanding Graduate Student, Shandong University, 2022</span><br><br>

  <span class="title">ACM SIGIR Student Travel Grant, 2022</span><br><br>

  <span class="title">Excellent Graduate, Hefei University of Technology, 2020</span><br><br>

  <span class="title">National Encouragement Scholarship, 2017, 2018, 2019</span><br><br>

  <span class="title">First Class Scholarship, Hefei University of Technology, 2018, 2019</span><br><br>

</tbody></table>
<!-- ============================Invited Talks===========================================!-->
</br>		

<!-- ============================Map===========================================!-->
<a href="https://clustrmaps.com/site/1bpzi"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=vuHwEKQroohZZfj1UB99qZ70x6e6FgHjRjzZ1ukd3I0&cl=ffffff" /></a>
<!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=vuHwEKQroohZZfj1UB99qZ70x6e6FgHjRjzZ1ukd3I0&cl=ffffff&w=a"></script> -->
<!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=300&t=tt&d=P5t3EabrzZY8aFh3ZhuRPAXXUh7jCpV3TVHKUlqbMjA&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script> -->
<!-- ============================Map===========================================! -->

</br>		
<p>Last Update, Oct 2024.</p> 
<!-- 	<p>Webpage template borrows from <a href="https://weiyinwei.github.io/">Yinwei Wei</a>.</p>  -->

</div>
</div>

</body>
</html>

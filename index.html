
<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta content="IE=7.0000" http-equiv="X-UA-Compatible">
<title>Liqiang Jing's Homepage</title>
<meta name="description" content="Liqian Jing.">
<meta name="keywords" content="Liqiang Jing, HFUT, SDU, UT Dallas, homepage, Ph.D.">

<style>@-moz-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-webkit-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@-o-keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}@keyframes nodeInserted{from{opacity:0.99;}to{opacity:1;}}embed,object{animation-duration:.001s;-ms-animation-duration:.001s;-moz-animation-duration:.001s;-webkit-animation-duration:.001s;-o-animation-duration:.001s;animation-name:nodeInserted;-ms-animation-name:nodeInserted;-moz-animation-name:nodeInserted;-webkit-animation-name:nodeInserted;-o-animation-name:nodeInserted;}</style>	
	
<link rel="stylesheet" type="text/css" href="./files/weiyinwei.css">
<meta name="google-site-verification" content="OU-xCmiAYHXy1Aj5mcXXaFv9VjXH0fn1X__CmSR6dUg" />

</head>


<body>
<div id="content">
<!-- ============================NEWS===========================================!-->
<div id="news">
    <h2>News</h2><br>
    <font size="3px">

    <b>Nov 2023</b><br>
    <span class="easylink">
    <b>  <a href="https://arxiv.org/pdf/2311.01477.pdf" target="_blank">FaithScore for LVLM hallucinations evaluation</a> </b> is out. Use our  <a href="https://pypi.org/project/faithscore/" target="_blank">tool</a> for any multimodal open-ended questions!
    </span><br><br>
	    
    <b>25 Sep 2023</b><br>
    <span class="easylink">
    Invited Reviewer for IEEE Transactions on Audio, Speech, and Language Processing.
    </span><br><br>
	    
    <b>1 Sep 2023</b><br>
    <span class="easylink">
    Invited Reviewer for ICLR 2024.
    </span><br><br>
	    
    <b>8 Aug 2023</b><br>
    <span class="easylink">
    Invited Emergency Reviewer for EMNLP 2023.
    </span><br><br>
	    
    <b>26 July 2023</b><br>
    <span class="easylink">
    Our full paper <b>"General Debiasing for Multimodal Sentiment Analysis"</b> is accepted by ACM MM.
    </span><br><br>
	    
    <b>14 Jun 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model"</b> is accepted by ACM TOIS.
    </span><br><br>
	    
    <b>27 May 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain"</b> is accepted by ACM TOIS.
    </span><br><br>
	    
    <b>2 May 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Multi-source Semantic Graph-based Multimodal Sarcasm Explanation Generation"</b> is accepted by ACL 2023.
    </span><br><br>
	    
<!--     <b>28 April 2023</b><br>
    <span class="easylink">
    I am very excited to pursue a PhD in Computer Science at the University of Texas at Dallas in the fall of 2023.
    </span><br><br> -->
	    
    <b>20 April 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Dual Consistency-enhanced Semi-supervised Sentiment Analysis towards COVID-19 Tweets"</b> is accepted by IEEE TKDE.
    </span><br><br>
	    
    <b>5 April 2023</b><br>
    <span class="easylink">
    Our full paper <b>"Adapting Generative Pretrained Language Model for Open-domain Multimodal Sentence Summarization"</b> is accepted by <a href="https://sigir.org/sigir2023/" target="_blank">ACM SIGIR 2023</a>.
    </span><br><br>
	    
    <b>28 March 2023</b><br>
    <span class="easylink">
    Invited Reviewer for NeurIPS 2023.
    </span><br><br>
	    
    <b>20 February 2023</b><br>
    <span class="easylink">
    Invited Reviewer for IEEE TCSVT.
    </span><br><br>
	    
    <b>29 January 2023</b><br>
    <span class="easylink">
    Invited Reviewer for ICML 2023.
    </span><br><br>
	    
    <b>17 January 2023</b><br>
    <span class="easylink">
    Invited Reviewer for IEEE TMM.
    </span><br><br>
	    
    <b>19 November 2022</b><br>
    <span class="easylink">
    Our paper <b>"Mutual-enhanced Incongruity Learning Network for Multi-modal Sarcasm Detection"</b> is accepted by <a href="https://aaai.org/Conferences/AAAI-23/" target="_blank">AAAI 2023 </a>
    </span><br><br>
	
    <b>20 October 2022</b><br>
    <span class="easylink">
    Invited Reviewer for Information Sciences.
    </span><br><br>
	    
    <b>30 August 2022</b><br>
    <span class="easylink">
    Our paper <b>"Vision Enhanced Generative Pre-trained Language Model for Multimodal Sentence Summarization"</b> is accepted by <a href="https://www.mi-research.net/" target="_blank">Machine Intelligence Research</a>.
    </span><br><br>
	    
    <b>30 July 2022</b><br>
    <span class="easylink">
    Our full paper <b>"CI-OCM: Counterfactural Inference towards Unbiased Outfit Compatibility Modeling"</b> is accepted by <a href="https://mcfr-mm22.github.io/" target="_blank">the Workshop MCFR of ACM MM 2022</a>.
    </span><br><br>
	    
    <b>24 July 2022</b><br>
    <span class="easylink">
    Invited Reviewer for NeurIPS 2022.
    </span><br><br>
	
    <b>17 July 2022</b><br>
    <span class="easylink">
    Invited Reviewer for ACM MM 2022.
    </span><br><br>
	    
    <b>30 June 2022</b><br>
    <span class="easylink">
    Our full paper <b>"Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis"</b> is accepted by <a href="https://2022.acmmm.org/" target="_blank">ACM MM 2022</a>.
    </span><br><br>
	    
    <b>28 April 2022</b><br>
    <span class="easylink">
    I won the <a href="https://sigir.org/sigir2022/" target="_blank">ACM SIGIR 2022</a> Student Travel Grant.
    </span><br><br>
	    
    <b>31 March 2022</b><br>
    <span class="easylink">
    Our full paper <b>"V2P: Vision-to-Prompt based Multi-Modal Product Summary Generation"</b> is accepted by <a href="https://sigir.org/sigir2022/" target="_blank">ACM SIGIR 2022</a>.
    </span><br><br>
	    
   <b>1 August 2021</b><br>
    <span class="easylink">
    I will be as a Research Intern at Alibaba DAMO Academy in Aug. 2021, supervised by Zhongzhou Zhao 
    </span><br><br>	    
    </font>
</div>
<!-- ============================Profiles===========================================!-->
<div id="left">
<table style="background-color:white;">
<tbody><tr nosave="">

	
<td valign="CENTER">
<img src="./images/profile.png" height="220" align="left">
<!-- <img src="./images/jingliqiang.jpg" height="220" align="left"> -->
</td>

<td valign="CENTER" align="left">
<font size="+0">
<b><font size="+2">Liqiang Jing （Chinese Name: 井立强）&nbsp;</font></b>
<p style="margin-left:0px;">
</p><p style="margin-left:0px;">
<!--<b>PHD Student</b>
</p><p style="margin-left:0px;">-->
<!-- <a href="http://ilearn.qd.sdu.edu.cn/", target="_blank">iLEARN</a><br/> -->
<!-- <a href="https://www.en.sdu.edu.cn/", target="_blank">Shandong University</a><br/> -->
<!-- </p><p style="margin-left:0px;">
The Hong Kong Polytechnic University, Hong Kong<br> -->
</p><p style="margin-left:0px;">
Email:&nbsp; jingliqiang6 AT gmail.com</a><br>
&bull; <a href="files/CV.pdf">CV</a> &bull; <a href="https://scholar.google.com/citations?hl=en&user=aNXRSOsAAAAJ">Google Scholar</a> &bull; <a href="https://github.com/LiqiangJing">GitHub</a> <br>
</p></font><p><font size="+0">
</font>
</p></td>
</tr>
</tbody></table>

<div style="margin-top:20px;">
Liqiang Jing is a Ph.D. student supervised by  <a href="https://xinyadu.github.io/" target="\_blank">Prof. Xinya Du</a> at the University of Texas at Dallas. 
<!-- Liqiang Jing is a Ph.D. student at the University of Texas at Dallas.  -->
He received the Master degree in the School of Computer Science and Technology from Shandong University in 2023 and the Bachelor degree in the School of Computer Science and Information from Hefei University of Technology 
in 2020. His research interests include multimodal learning and natural language processing. Now, he is very interested in safety for large vision-language models and large language models.
<!-- He has published several papers in the top venues (e.g., ACM SIGIR, ACM MM, AAAI, ACL, and IEEE TKDE). 
Moreover, he has served as a reviewer for several top conferences and journals, such as NeurIPS, TMM, ICML, TCSVT, ICLR, and Information Sciences. -->
	
</div>
<!-- ============================Profiles===========================================!-->

<div>
	<p><font color='red'> I am happy to chat and discuss potential collaborations. Please feel free to reach out to me via Email (jingliqiang6 AT gmail.com). 
		</font></p>
</div>


<!-- ============================Education===========================================!-->
<h2 style="CLEAR: both;">Education</h2>
	<table>
  <tbody>
  <tr>
    <td><span class="title">The University of Texas at Dallas</span> <br>
	Ph.D. in Computer Science, Aug. 2023 - present  <br>
	Advisor: <a href="https://xinyadu.github.io/" target="_blank">Xinya Du</a> <br>
     </td>
   </tr>
   </tbody>
</table>     
	
<table>
  <tbody>
  <tr>
    <td><span class="title">Shandong University </span> <br>
	Master in Computer Technology, Sep. 2020 - Jun. 2023  <br>
	 <a href="papers/trans1.pdf" target="_blank">Transcript</a>  <br>
	Advisor: <a href="https://xuemengsong.github.io/" target="_blank">Xuemeng Song</a> <br>
	Co-Advisor: <a href="https://liqiangnie.github.io/index.html" target="_blank">Liqiang Nie</a> 
     </td>
   </tr>
   </tbody>
</table>     

<table>
  <tbody>
  <tr>
    <td><span class="title">Hefei University of Technology </span> <br>
	Bachelor in Computer Science and Technology, Sep. 2016 - Jul. 2020 <br>
	    <a href="papers/trans2.pdf" target="_blank">Transcript</a> <br>
     </td>
   </tr>
   </tbody>
</table>


<!-- ============================Experiences===========================================!-->
<h2 style="CLEAR: both">Experiences</h2>
<table>
  <tbody><tr>
    <td> <span class="title">Research Intern</span>, Alibaba DAMO Academy, May. 2022 -- Jan. 2023<br>
			Advisor:  <a href="https://dblp.uni-trier.de/pid/207/9948.html" target="_blank">Zhongzhou Zhao</a>
		</td></tr></tbody>
</table>

<table>
  <tbody><tr>
    <td> <span class="title">AIR <a href="https://damo.alibaba.com/collaborations/?lang=en" target="_blank">(Alibaba Innovative Research)</a> Project Intern</span>, Alibaba DAMO Academy, Aug. 2021 -- Jan. 2022 <br>
			Advisor:  <a href="https://dblp.uni-trier.de/pid/207/9948.html" target="_blank">Zhongzhou Zhao</a>
		</td></tr></tbody>
</table>



<!-- ============================Experiences===========================================!-->


<!-- ============================Papers===========================================!-->

	<div id="papers">
<h2 style="CLEAR: both">Preprints <a href="" target="_blank"></a></h2> 
<table>
  <tbody>
	  
<tr><td class="left"><a href="https://arxiv.org/abs/2311.01477" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">FaithScore: Evaluating Hallucinations in Large Vision-Language Models.</span> 
      <br><b>Liqiang Jing</b>, Ruosen Li, Yunmo Chen, Mengzhao Jia & Xinya Du
    <br> Arxiv
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>
    
<tr><td class="left"><a href="https://arxiv.org/abs/2310.07700" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">Knowledge-enhanced Memory Model for Emotional Support Conversation.</span> 
      <br>Mengzhao Jia, Qianglong Chen, <b>Liqiang Jing</b>, Dawei Fu & Renyu Li
    <br> Arxiv
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>
    
	  
 </tbody>
</table>
	

		
		
<div id="papers">
<h2 style="CLEAR: both">Publications <a href="" target="_blank"></a></h2> 
</br>
	
	<b> In the Year of 2023: </b> </br></br>
<table>
  <tbody>
	<tr>
	  <td class="left"><a href="https://link.springer.com/article/10.1007/s11633-022-1372-x" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Vision Enhanced Generative Pre-trained Language Model for Multimodal Sentence Summarization</span> 
      <br> <b>Liqiang Jing</b>, Yiren Li, Junhao Xu, Yongcan Yu, Pei Shen & Xuemeng Song
    <br>Machine Intelligence Research
   &nbsp;&nbsp;&bull; <a href="https://github.com/LiqiangJing/Vision-GPLM" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
  </tr>  
	  
	  <tr>
	<td class="left"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/26138" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Mutual-enhanced Incongruity Learning Network for Multi-modal Sarcasm Detection.</span> 
      <br>Yang Qiao,  <b>Liqiang Jing</b>, Xuemeng Song, Xiaolin Chen, Lei Zhu & Liqiang Nie
    <br>AAAI 2023
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
	  </tr>
    
	  <tr>
	<td class="left"><a href="https://dl.acm.org/doi/abs/10.1145/3539618.3591633" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Adapting Generative Pretrained Language Model for Open-domain Multimodal Sentence Summarization.</span> 
      <br>Dengtian Lin,  <b>Liqiang Jing</b>, Xuemeng Song, Meng Liu, Teng Sun & Liqiang Nie
    <br>ACM SIGIR 2023
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
	  </tr>
	  
	  	  <tr>
	<td class="left"><a href="https://ieeexplore.ieee.org/document/10109879" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Dual Consistency-enhanced Semi-supervised Sentiment Analysis towards COVID-19 Tweets.</span> 
      <br>Teng Sun,  <b>Liqiang Jing</b>, Yinwei Wei, Xuemeng Song, Zhiyong Cheng & Liqiang Nie
    <br>IEEE TKDE
   &nbsp;&nbsp;&bull; <a href="https://ieeexplore.ieee.org/document/10109879" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
	  </tr>
	  

	  	  	  <tr>
	<td class="left"><a href="https://aclanthology.org/2023.acl-long.635/" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Multi-source Semantic Graph-based Multimodal Sarcasm Explanation Generation.</span> 
      <br><b>Liqiang Jing</b>, Xuemeng Song, Kun Ouyang, Mengzhao Jia & Liqiang Nie
    <br>ACL 2023
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
	  </tr>
	  
	<tr><td class="left"><a href="https://dl.acm.org/doi/10.1145/3603374" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain.</span> 
      <br><b>Liqiang Jing</b>, Xuemeng Song, Xuming Lin, Zhongzhou Zhao, Wei Zhou & Liqiang Nie
    <br> ACM TOIS
   &nbsp;&nbsp;&bull; <a href="https://dl.acm.org/doi/10.1145/3603374" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>
	  
	<tr><td class="left"><a href="https://dl.acm.org/doi/pdf/10.1145/3606368" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model.</span> 
      <br>Xiaolin Chen, Xuemeng Song, <b>Liqiang Jing</b>, Shuo Li, Linmei Hu & Liqiang Nie
    <br>ACM TOIS
   &nbsp;&nbsp;&bull; <a href="https://multimodaldialog.wixsite.com/website" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>

	<tr><td class="left"><a href="https://arxiv.org/abs/2307.10511" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
   <td><span class="title">General Debiasing for Multimodal Sentiment Analysis.</span> 
      <br>Teng Sun, Juntong Ni, Wenjie Wang, <b>Liqiang Jing</b>, Yinwei Wei & Liqiang Nie
    <br>ACM MM 2023
   &nbsp;&nbsp;&bull; <a href="" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
</tr>
	  
	  
	  
 </tbody>
</table>




<b> In the Year of 2022: </b> </br></br>
<table>
  <tbody>  
  <tr>    
    <td class="left"><a href="https://dl.acm.org/doi/abs/10.1145/3477495.3532076" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">V2P: Vision-to-Prompt based Multi-Modal Product Summary Generation</span> 
      <br> Xuemeng Song, <b>Liqiang Jing</b>, Dengtian Lin, Zhongzhou Zhao, Haiqing Chen & Liqiang Nie
    <br>ACM SIGIR 2022
   &nbsp;&nbsp;&bull; <a href="https://xuemengsong.github.io/V2P_Code.rar" target="_blank">Codes&Data</a> &nbsp;&nbsp;

  </td>
  </tr>
	 
	    <tr>
	      <td class="left"><a href="https://arxiv.org/abs/2207.11652" 
			target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis</span> 
      <br> Teng Sun, Wenjie Wang, <b>Liqiang Jing</b>, Yiran Cui, Xuemeng Song & Liqiang Nie
    <br>ACM MM 2022
   &nbsp;&nbsp;&bull; <a href="https://github.com/Teng-Sun/CLUE_model" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
  </tr>
	     <tr>
	   <td class="left"><a href="https://dl.acm.org/doi/abs/10.1145/3552468.3555363" target="_blank"><img src="./images/pdf.png" width="25" height="25"><br>pdf</a></td>
    <td><span class="title">CI-OCM: Counterfactural Inference towards Unbiased Outfit Compatibility Modeling</span> 
      <br> <b>Liqiang Jing</b>, Minghui Tian, Xiaolin Chen, Teng Sun, Weili Guan & Xuemeng Song
    <br>ACM MM 2022
   &nbsp;&nbsp;&bull; <a href="https://github.com/LiqiangJing/CI-OCM" target="_blank">Codes&Data</a> &nbsp;&nbsp;
  </td>
  </tr>
  
 </tbody>
</table>



<!-- ============================Patent===========================================!-->
<!-- <h2 style="CLEAR: both;">Patent</h2> -->







</td></tr></tbody></table>

<h2 style="CLEAR: both;">Presentations</h2>
	
<table><tbody><tr><td>
  <span class="title">ACM SIGIR 2022 Oral on V2P: Vision-to-Prompt based Multi-Modal Product Summary Generation</span> &nbsp;&nbsp; 
  <br> <a href="papers/v2p1.pdf" target="_blank">Slides</a>
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">ACM MM 2022 Oral on Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis</span> &nbsp;&nbsp; 
  <br> <a href="papers/msa1.pdf" target="_blank">Slides</a>
</td></tr></tbody></table>
	
<table><tbody><tr><td>
  <span class="title">ACM MCFR 2022 on CI-OCM: Counterfactual Inference towards Unbiased Outfit Compatibility Modeling</span> &nbsp;&nbsp; 
  <br> <a href="papers/ocm1.pdf" target="_blank">Slides</a>
</td></tr></tbody></table>

<!-- ============================Patent===========================================!-->

<h2 style="CLEAR: both;">Patent</h2>

<table><tbody><tr><td>
  <span class="title">Text generation methods and devices (applying) CN202211048016.0 &nbsp;&nbsp;
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">Text generation methods and devices (applying) CN202211158537.1 &nbsp;&nbsp;
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title"> A knowledge-guided multi-source information fusion method for predicting blast furnace gas (applying) CN202210606561.0 &nbsp;&nbsp;
</td></tr></tbody></table>

<table><tbody><tr><td>
  <span class="title">Adaptive deployment methods, systems, devices and storage media of transmission inspection terminals CN202211146873.4 &nbsp;&nbsp;
</td></tr></tbody></table>

<!-- ============================TA===========================================!-->
<h2 style="CLEAR: both;">Teacher Assistant</h2>
<table><tbody><tr><td>

	<span class="title">Fall 2023, <a href="https://xinyadu.github.io/cs6320/index.html" target="_blank">Natural Language Processing</a>, University of Texas at Dallas</span><br><br>

  <span class="title">Spring 2021, Machine Learning, Shandong University</span><br><br>

</tbody></table>
	

<!-- ============================Professional Service===========================================!-->
<h2 style="CLEAR: both;">Professional Services</h2>

<table><tbody>
	<tr><td>
	<b>Invited Reviewer for Conferences: ACM MM Workshop 2022, NeurIPS 2022, ICML 2023, NeurIPS 2023, EMNLP 2023, ICLR 2024.</b>
	</td></tr>
	<tr><td>
	<b>Invited Reviewer for Journals: Information Sciences, IEEE TMM, IEEE TCSVT, IEEE TASLP.</b>
	</td></tr>
</tbody></table>


<!-- ============================Honors===========================================!-->
<h2 style="CLEAR: both;">Honors</h2>
<table><tbody><tr><td>
	
  <span class="title">Outstanding Graduate Student, Shandong University, 2022</span><br><br>

  <span class="title">ACM SIGIR Student Travel Grant, 2022</span><br><br>

  <span class="title">Excellent Graduate, Hefei University of Technology, 2020</span><br><br>

  <span class="title">National Encouragement Scholarship, 2017, 2018, 2019</span><br><br>

  <span class="title">First Class Scholarship, Hefei University of Technology, 2018, 2019</span><br><br>

</tbody></table>
<!-- ============================Invited Talks===========================================!-->
</br>		

<!-- ============================Map===========================================!-->
<a href="https://clustrmaps.com/site/1bpzi"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=vuHwEKQroohZZfj1UB99qZ70x6e6FgHjRjzZ1ukd3I0&cl=ffffff" /></a>
<!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=vuHwEKQroohZZfj1UB99qZ70x6e6FgHjRjzZ1ukd3I0&cl=ffffff&w=a"></script> -->
<!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=300&t=tt&d=P5t3EabrzZY8aFh3ZhuRPAXXUh7jCpV3TVHKUlqbMjA&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script> -->
<!-- ============================Map===========================================! -->

</br>		
<p>Last Update, 10, Sep 2023.</p> 
	<p>Webpage template borrows from <a href="https://weiyinwei.github.io/">Yinwei Wei</a>.</p> 

</div>
</div>

</body>
</html>
